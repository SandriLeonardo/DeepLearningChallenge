{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11958675,"sourceType":"datasetVersion","datasetId":7519186},{"sourceId":11986724,"sourceType":"datasetVersion","datasetId":7519473}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":108.457758,"end_time":"2025-05-21T16:25:04.482169","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-21T16:23:16.024411","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Graph Neural Network Training Pipeline","metadata":{}},{"cell_type":"markdown","source":"Multi-Dataset Graph Classification with Noise-Robust Training","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"id":"xSkgt1zf-raF","outputId":"59f4a52f-5eb4-41e5-9fba-07432989fe78","papermill":{"duration":5.620104,"end_time":"2025-05-21T16:23:24.36169","exception":false,"start_time":"2025-05-21T16:23:18.741586","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:25.281543Z","iopub.execute_input":"2025-05-29T12:35:25.281773Z","iopub.status.idle":"2025-05-29T12:35:30.662622Z","shell.execute_reply.started":"2025-05-29T12:35:25.281756Z","shell.execute_reply":"2025-05-29T12:35:30.661772Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import random_split\nimport argparse","metadata":{"id":"lAQuCuIoBbq5","papermill":{"duration":9.949638,"end_time":"2025-05-21T16:25:02.510764","exception":false,"start_time":"2025-05-21T16:24:52.561126","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:30.664076Z","iopub.execute_input":"2025-05-29T12:35:30.664350Z","iopub.status.idle":"2025-05-29T12:35:40.602137Z","shell.execute_reply.started":"2025-05-29T12:35:30.664324Z","shell.execute_reply":"2025-05-29T12:35:40.601532Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"helper_scripts_path = '/kaggle/input/d/leonardosandri/myhackatonhelperscripts/'\n\nif os.path.exists(helper_scripts_path):\n    # Add this path to the beginning of Python's search list\n    sys.path.insert(0, helper_scripts_path)\n    print(f\"Successfully added '{helper_scripts_path}' to sys.path.\")\n    print(f\"Contents of '{helper_scripts_path}': {os.listdir(helper_scripts_path)}\") # Verify\nelse:\n    print(f\"WARNING: Helper scripts path not found: {helper_scripts_path}\")\n    print(\"Please ensure 'myhackathonhelperscripts' dataset is correctly added to the notebook.\")\n\n# Start import of utils modules\ntry:\n    from preprocessor import MultiDatasetLoader\n    from utils import set_seed\n    # from conv import GINConv as OriginalRepoGINConv\n    from models_EDandBatch_norm import GNN\n    print(\"Successfully imported modules.\")\nexcept ImportError as e:\n    print(f\"ERROR importing module: {e}\")\n    print(\"Please check that the .py files exist directly under the helper_scripts_path and have no syntax errors.\")\n    # print(\"Current sys.path:\", sys.path)\n\n# Set the random seed\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:40.603747Z","iopub.execute_input":"2025-05-29T12:35:40.604382Z","iopub.status.idle":"2025-05-29T12:35:41.374572Z","shell.execute_reply.started":"2025-05-29T12:35:40.604360Z","shell.execute_reply":"2025-05-29T12:35:41.373828Z"}},"outputs":[{"name":"stdout","text":"Successfully added '/kaggle/input/d/leonardosandri/myhackatonhelperscripts/' to sys.path.\nContents of '/kaggle/input/d/leonardosandri/myhackatonhelperscripts/': ['models_edge_drop.py', 'zipthefolder.py', 'loadData.py', 'utils.py', 'models_EDandBatch_norm.py', 'models.py', 'conv.py', 'preprocessor.py', '__init__.py']\nSuccessfully imported modules.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2. Data Preprocessing Functions\n","metadata":{}},{"cell_type":"code","source":"def add_zeros(data):\n    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n    return data","metadata":{"id":"Dyf0I2-t9IcW","papermill":{"duration":0.019268,"end_time":"2025-05-21T16:25:02.544583","exception":false,"start_time":"2025-05-21T16:25:02.525315","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.376375Z","iopub.execute_input":"2025-05-29T12:35:41.376775Z","iopub.status.idle":"2025-05-29T12:35:41.380469Z","shell.execute_reply.started":"2025-05-29T12:35:41.376756Z","shell.execute_reply":"2025-05-29T12:35:41.379826Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 3. Training and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch, scheduler=None, args=None):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_idx, data in enumerate(tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\")):\n        data = data.to(device)\n        optimizer.zero_grad()\n        \n        try:\n            output = model(data)\n        except IndexError as e:\n            print(f\"Error in batch with {data.num_nodes} nodes, edge_max={data.edge_index.max()}\")\n            print(f\"Batch info: x.shape={data.x.shape}, edge_index.shape={data.edge_index.shape}\")\n            raise e\n            \n        loss = criterion(output, data.y)\n        loss.backward()\n        optimizer.step()\n\n        # Step OneCycleLR scheduler after each batch\n        if scheduler is not None and args.scheduler_type == 'OneCycleLR':\n            scheduler.step()\n            \n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total += data.y.size(0)\n\n    # Save checkpoints if required\n    if save_checkpoints:\n        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n        torch.save(model.state_dict(), checkpoint_file)\n        print(f\"Checkpoint saved at {checkpoint_file}\")\n\n    return total_loss / len(data_loader),  correct / total","metadata":{"id":"3jKvoQYI9Zbc","papermill":{"duration":0.019599,"end_time":"2025-05-21T16:25:02.577661","exception":false,"start_time":"2025-05-21T16:25:02.558062","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.381453Z","iopub.execute_input":"2025-05-29T12:35:41.381734Z","iopub.status.idle":"2025-05-29T12:35:41.396582Z","shell.execute_reply.started":"2025-05-29T12:35:41.381711Z","shell.execute_reply":"2025-05-29T12:35:41.395912Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# CELL 7 (Corrected)\ndef evaluate(data_loader, model, criterion, device, calculate_accuracy=False): # Added 'criterion' argument\n    model.eval()\n    correct = 0\n    total = 0\n    predictions = []\n    total_loss = 0\n    # REMOVE THE HARDCODED CRITERION:\n    # criterion = torch.nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n            data = data.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n\n            if calculate_accuracy:\n                correct += (pred == data.y).sum().item()\n                total += data.y.size(0)\n                # NOW USES THE PASSED-IN CRITERION\n                total_loss += criterion(output, data.y).item()\n            else:\n                predictions.extend(pred.cpu().numpy())\n    if calculate_accuracy:\n        accuracy = correct / total\n        return  total_loss / len(data_loader), accuracy # Ensure consistent return order\n    return predictions","metadata":{"id":"8peFiIS19ZpK","papermill":{"duration":0.017908,"end_time":"2025-05-21T16:25:02.607848","exception":false,"start_time":"2025-05-21T16:25:02.58994","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.397195Z","iopub.execute_input":"2025-05-29T12:35:41.397463Z","iopub.status.idle":"2025-05-29T12:35:41.414634Z","shell.execute_reply.started":"2025-05-29T12:35:41.397447Z","shell.execute_reply":"2025-05-29T12:35:41.413962Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 4. Utility Functions","metadata":{}},{"cell_type":"code","source":"def save_predictions(predictions, test_path):\n    script_dir = os.getcwd() \n    submission_folder = os.path.join(script_dir, \"submission\")\n    test_dir_name = os.path.basename(os.path.dirname(test_path))\n    \n    os.makedirs(submission_folder, exist_ok=True)\n    \n    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n    \n    test_graph_ids = list(range(len(predictions)))\n    output_df = pd.DataFrame({\n        \"id\": test_graph_ids,\n        \"pred\": predictions\n    })\n    \n    output_df.to_csv(output_csv_path, index=False)\n    print(f\"Predictions saved to {output_csv_path}\")","metadata":{"id":"WanuZKxy9Zs-","papermill":{"duration":0.016728,"end_time":"2025-05-21T16:25:02.635694","exception":false,"start_time":"2025-05-21T16:25:02.618966","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.415359Z","iopub.execute_input":"2025-05-29T12:35:41.415565Z","iopub.status.idle":"2025-05-29T12:35:41.431276Z","shell.execute_reply.started":"2025-05-29T12:35:41.415551Z","shell.execute_reply":"2025-05-29T12:35:41.430774Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def plot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n    \"\"\"\n    Plot training and validation progress over epochs.\n    \n    Args:\n        train_losses: List of training losses per epoch\n        train_accuracies: List of training accuracies per epoch  \n        val_losses: List of validation losses per epoch\n        val_accuracies: List of validation accuracies per epoch\n        learning_rates: List of learning rates per epoch\n        output_dir: Directory to save the plot\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    \n    # Create figure with 3 subplots\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n        \n    # Plot losses\n    ax1.plot(epochs, train_losses, label=\"Training Loss\", color='blue', marker='o', markersize=3)\n    ax1.plot(epochs, val_losses, label=\"Validation Loss\", color='red', marker='s', markersize=3)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Validation Loss per Epoch')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot accuracies\n    ax2.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green', marker='o', markersize=3)\n    ax2.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color='orange', marker='s', markersize=3)\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Training and Validation Accuracy per Epoch')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # Plot learning rate\n    ax3.plot(epochs, learning_rates, label=\"Learning Rate\", color='purple', marker='d', markersize=3)\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Learning Rate')\n    ax3.set_title('Learning Rate Schedule')\n    ax3.set_yscale('log')  # Use log scale for better visualization\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Save plot\n    os.makedirs(output_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"training_progress_with_lr.png\"), dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n\n    # Create a summary plot showing best epochs\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    \n    # Plot validation accuracy and learning rate on same plot with different y-axes\n    ax_lr = ax.twinx()\n    \n    line1 = ax.plot(epochs, val_accuracies, 'b-', label='Validation Accuracy', linewidth=2, alpha=0.8)\n    line2 = ax_lr.plot(epochs, learning_rates, 'r--', label='Learning Rate', linewidth=2, alpha=0.8)\n    \n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Validation Accuracy', color='b')\n    ax_lr.set_ylabel('Learning Rate', color='r')\n    ax_lr.set_yscale('log')\n    \n    # Find and mark best validation accuracy\n    best_epoch = epochs[val_accuracies.index(max(val_accuracies))]\n    best_acc = max(val_accuracies)\n    ax.scatter([best_epoch], [best_acc], color='gold', s=100, zorder=5, \n               label=f'Best: Epoch {best_epoch}, Acc: {best_acc:.4f}')\n    \n    ax.set_title('Validation Accuracy vs Learning Rate Schedule')\n    ax.grid(True, alpha=0.3)\n    \n    # Combine legends\n    lines = line1 + line2 + ax.collections\n    labels = [l.get_label() for l in lines]\n    ax.legend(lines, labels, loc='center left')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"accuracy_vs_lr.png\"), dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()","metadata":{"id":"uyHIJS5U9ZzB","papermill":{"duration":0.017765,"end_time":"2025-05-21T16:25:02.664538","exception":false,"start_time":"2025-05-21T16:25:02.646773","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.431907Z","iopub.execute_input":"2025-05-29T12:35:41.432090Z","iopub.status.idle":"2025-05-29T12:35:41.447172Z","shell.execute_reply.started":"2025-05-29T12:35:41.432058Z","shell.execute_reply":"2025-05-29T12:35:41.446662Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 5. Configuration and Arguments","metadata":{}},{"cell_type":"code","source":"def get_user_input(prompt, default=None, required=False, type_cast=str):\n\n    while True:\n        user_input = input(f\"{prompt} [{default}]: \")\n        \n        if user_input == \"\" and required:\n            print(\"This field is required. Please enter a value.\")\n            continue\n        \n        if user_input == \"\" and default is not None:\n            return default\n        \n        if user_input == \"\" and not required:\n            return None\n        \n        try:\n            return type_cast(user_input)\n        except ValueError:\n            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")","metadata":{"papermill":{"duration":0.016577,"end_time":"2025-05-21T16:25:02.692205","exception":false,"start_time":"2025-05-21T16:25:02.675628","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.447916Z","iopub.execute_input":"2025-05-29T12:35:41.448178Z","iopub.status.idle":"2025-05-29T12:35:41.465193Z","shell.execute_reply.started":"2025-05-29T12:35:41.448158Z","shell.execute_reply":"2025-05-29T12:35:41.464578Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_arguments():\n    \"\"\"Set training configuration directly\"\"\"\n    args = {\n        # Dataset selection\n        'dataset': 'A',  # Choose: A, B, C, D\n        'train_mode': 1,  # 1=single dataset, 2=all datasets\n        \n        # Model config\n        #'gnn': 'gin',  # gin, gin-virtual, gcn, gcn-virtual\n        'num_layer': 3,\n        'emb_dim': 256,\n        'drop_ratio': 0.3,   # Dropout ratio\n        'virtual_node': True, # True to use virtual node, False otherwise\n        'residual': True,    # True to use residual connections, False otherwise\n        'JK': \"last\",         # Jumping Knowledge: \"last\", \"sum\", \"cat\"\n        'edge_drop_ratio' : 0.15,\n        'batch_norm' : True,\n        'graph_pooling': \"mean\", # \"sum\", \"mean\", \"max\", \"attention\", \"set2set\"\n        \n        # Training config\n        'batch_size': 64,\n        'epochs': 200,\n        'baseline_mode': 3,  # 1=CE, 2=Noisy CE, 3 GCE\n        'noise_prob': 0.2,\n        'gce_q' : 0.5,\n        'initial_lr' : 1e-3,\n\n        # Lr scheduler config =================================================================================================================\n        'use_scheduler' : True,\n        'scheduler_type': 'ReduceLROnPlateau',  # Options: 'StepLR', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'ExponentialLR', 'OneCycleLR'\n\n        # StepLR parameters\n        'step_size': 30,      # Period of learning rate decay for StepLR\n        'gamma': 0.5,         # Multiplicative factor of learning rate decay\n        \n        # ReduceLROnPlateau parameters\n        'patience_lr': 10,    # Number of epochs with no improvement after which LR will be reduced\n        'factor': 0.5,        # Factor by which the learning rate will be reduced\n        'min_lr': 1e-7,       # Lower bound on the learning rate\n        \n        # CosineAnnealingLR parameters\n        'T_max': 50,          # Maximum number of iterations for cosine annealing\n        'eta_min': 1e-6,      # Minimum learning rate\n        \n        # ExponentialLR parameters\n        'gamma_exp': 0.95,    # Multiplicative factor of learning rate decay for ExponentialLR\n        \n        # OneCycleLR parameters\n        'max_lr': 1e-3,       # Upper learning rate boundary\n        'pct_start': 0.3,     # Percentage of cycle spent increasing learning rate\n\n        # =====================================================================================================================================\n        \n        # Early stopping config\n        'early_stopping': True,  # Enable/disable early stopping\n        'patience': 25,          # Number of epochs to wait without improvement\n        \n        # System config\n        'device': 0,\n        'num_checkpoints': 3,\n    }\n    return argparse.Namespace(**args)","metadata":{"papermill":{"duration":0.017703,"end_time":"2025-05-21T16:25:02.721184","exception":false,"start_time":"2025-05-21T16:25:02.703481","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.467169Z","iopub.execute_input":"2025-05-29T12:35:41.467403Z","iopub.status.idle":"2025-05-29T12:35:41.479926Z","shell.execute_reply.started":"2025-05-29T12:35:41.467388Z","shell.execute_reply":"2025-05-29T12:35:41.479363Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def populate_args(args):\n    print(\"Arguments received:\")\n    for key, value in vars(args).items():\n        print(f\"{key}: {value}\")\nargs = get_arguments()\npopulate_args(args)","metadata":{"papermill":{"duration":0.118164,"end_time":"2025-05-21T16:25:02.850799","exception":true,"start_time":"2025-05-21T16:25:02.732635","status":"failed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.480786Z","iopub.execute_input":"2025-05-29T12:35:41.481116Z","iopub.status.idle":"2025-05-29T12:35:41.497035Z","shell.execute_reply.started":"2025-05-29T12:35:41.481066Z","shell.execute_reply":"2025-05-29T12:35:41.496470Z"}},"outputs":[{"name":"stdout","text":"Arguments received:\ndataset: A\ntrain_mode: 1\nnum_layer: 3\nemb_dim: 256\ndrop_ratio: 0.3\nvirtual_node: True\nresidual: True\nJK: last\nedge_drop_ratio: 0.15\nbatch_norm: True\ngraph_pooling: mean\nbatch_size: 64\nepochs: 200\nbaseline_mode: 3\nnoise_prob: 0.2\ngce_q: 0.5\ninitial_lr: 0.001\nuse_scheduler: True\nscheduler_type: ReduceLROnPlateau\nstep_size: 30\ngamma: 0.5\npatience_lr: 10\nfactor: 0.5\nmin_lr: 1e-07\nT_max: 50\neta_min: 1e-06\ngamma_exp: 0.95\nmax_lr: 0.001\npct_start: 0.3\nearly_stopping: True\npatience: 25\ndevice: 0\nnum_checkpoints: 3\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 6. Loss Function Definition","metadata":{}},{"cell_type":"code","source":"class NoisyCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, p_noisy):\n        super().__init__()\n        self.p = p_noisy\n        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        losses = self.ce(logits, targets)\n        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n        return (losses * weights).mean()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.497712Z","iopub.execute_input":"2025-05-29T12:35:41.497925Z","iopub.status.idle":"2025-05-29T12:35:41.511194Z","shell.execute_reply.started":"2025-05-29T12:35:41.497901Z","shell.execute_reply":"2025-05-29T12:35:41.510692Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# CELL 12.1 (New Cell or append to existing cell 12)\nclass GeneralizedCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, q=0.7):\n        \"\"\"\n        Generalized Cross Entropy Loss.\n        q is a hyperparameter, 0 < q <= 1.\n        As q -> 0, GCE approaches standard CE.\n        \"\"\"\n        super(GeneralizedCrossEntropyLoss, self).__init__()\n        if not (0 < q <= 1):\n            # While the limit q->0 is CE, for q=0 direct computation is 1/0.\n            # The paper usually uses q > 0.\n            raise ValueError(\"q should be in (0, 1]\")\n        self.q = q\n\n    def forward(self, logits, targets):\n        probs = torch.softmax(logits, dim=1)\n        # Select probabilities of the target class for each sample\n        target_probs = probs[torch.arange(targets.size(0)), targets]\n\n        # To prevent issues with target_probs being exactly 0,\n        # especially if q is very small (though here q > 0).\n        # However, 0^q is 0 for q > 0, so it should be fine.\n        # For extra safety: target_probs = target_probs.clamp(min=1e-8)\n\n        # GCE loss: (1 - p_t^q) / q\n        loss = (1 - (target_probs ** self.q)) / self.q\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.511910Z","iopub.execute_input":"2025-05-29T12:35:41.512146Z","iopub.status.idle":"2025-05-29T12:35:41.525795Z","shell.execute_reply.started":"2025-05-29T12:35:41.512124Z","shell.execute_reply":"2025-05-29T12:35:41.525175Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## 7. Model creation","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Config section\n","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"Enhanced GNN Training Pipeline\")\nprint(\"=\" * 60)\n\n# Get configuration\nargs = get_arguments()\n\nprint(\"\\nConfiguration:\")\nfor key, value in vars(args).items():\n    print(f\"  {key}: {value}\")\n\n# Setup device\ndevice = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nUsing device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.526485Z","iopub.execute_input":"2025-05-29T12:35:41.526705Z","iopub.status.idle":"2025-05-29T12:35:41.544578Z","shell.execute_reply.started":"2025-05-29T12:35:41.526682Z","shell.execute_reply":"2025-05-29T12:35:41.543874Z"}},"outputs":[{"name":"stdout","text":"============================================================\nEnhanced GNN Training Pipeline\n============================================================\n\nConfiguration:\n  dataset: A\n  train_mode: 1\n  num_layer: 3\n  emb_dim: 256\n  drop_ratio: 0.3\n  virtual_node: True\n  residual: True\n  JK: last\n  edge_drop_ratio: 0.15\n  batch_norm: True\n  graph_pooling: mean\n  batch_size: 64\n  epochs: 200\n  baseline_mode: 3\n  noise_prob: 0.2\n  gce_q: 0.5\n  initial_lr: 0.001\n  use_scheduler: True\n  scheduler_type: ReduceLROnPlateau\n  step_size: 30\n  gamma: 0.5\n  patience_lr: 10\n  factor: 0.5\n  min_lr: 1e-07\n  T_max: 50\n  eta_min: 1e-06\n  gamma_exp: 0.95\n  max_lr: 0.001\n  pct_start: 0.3\n  early_stopping: True\n  patience: 25\n  device: 0\n  num_checkpoints: 3\n\nUsing device: cuda:0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### 7.2 Data Loading","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"LOADING DATA\")\nprint(\"=\"*40)\n\nbase_path = '/kaggle/input/deep-dataset-preprocessed/processed_data_separate'\n\n# Prepare training/validation data based on mode\nif args.train_mode == 1:\n    # Single dataset mode\n    dataset_name = args.dataset\n    train_dataset = torch.load(f'{base_path}/{dataset_name}_train_graphs.pt', weights_only=False)\n    train_dataset = [add_zeros(data) for data in train_dataset]\n    \n    val_dataset = torch.load(f'{base_path}/{dataset_name}_val_graphs.pt', weights_only=False)\n    val_dataset = [add_zeros(data) for data in val_dataset]\n    \n    test_dataset = torch.load(f'{base_path}/{dataset_name}_test_graphs.pt', weights_only=False)\n    test_dataset = [add_zeros(data) for data in test_dataset]\n    print(f\"Using single dataset: {dataset_name}\")\nelse:\n    # All datasets mode\n    train_dataset = []\n    val_dataset = []\n    test_dataset = torch.load(f'{base_path}/{args.dataset}_test_graphs.pt', weights_only=False)  # Test on specified dataset\n    \n    for ds_name in ['A', 'B', 'C', 'D']:\n        train_dataset.extend(torch.load(f'{base_path}/{ds_name}_train_graphs.pt', weights_only=False))\n        val_dataset.extend(torch.load(f'{base_path}/{ds_name}_val_graphs.pt', weights_only=False))\n    \n    print(\"Using all datasets for training\")\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:35:41.545420Z","iopub.execute_input":"2025-05-29T12:35:41.545866Z","iopub.status.idle":"2025-05-29T12:36:10.130132Z","shell.execute_reply.started":"2025-05-29T12:35:41.545841Z","shell.execute_reply":"2025-05-29T12:36:10.129383Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nLOADING DATA\n========================================\nUsing single dataset: A\nTrain samples: 10152\nVal samples: 1128\nTest samples: 2340\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 7.3 Model Setup","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*40)\nprint(\"MODEL SETUP\")\nprint(\"=\"*40)\n\n# Initialize model\nmodel = GNN(num_class=6, # Assuming 6 classes based on original notebook\n            num_layer=args.num_layer,\n            emb_dim=args.emb_dim,\n            drop_ratio=args.drop_ratio,\n            virtual_node=args.virtual_node,\n            residual=args.residual,\n            JK=args.JK,\n            graph_pooling=args.graph_pooling,\n            edge_drop_ratio = args.edge_drop_ratio,\n            batch_norm=args.batch_norm\n           )\n\nmodel = model.to(device)\n\n# Setup optimizer and loss\noptimizer = torch.optim.Adam(model.parameters(), lr=args.initial_lr)\n\nif args.baseline_mode == 2:\n    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n    print(f\"Using Noisy Cross Entropy Loss (p={args.noise_prob})\")\nelif args.baseline_mode == 3: # <--- ADD THIS BLOCK FOR GCE\n    criterion = GeneralizedCrossEntropyLoss(q=args.gce_q)\n    print(f\"Using Generalized Cross Entropy (GCE) Loss (q={args.gce_q})\")\nelse:\n    criterion = torch.nn.CrossEntropyLoss()\n    print(\"Using standard Cross Entropy Loss\")\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Setup logging and checkpoints\nexp_name = f\"gin_dataset{args.dataset}_mode{args.train_mode}\"\nlogs_dir = os.path.join(\"logs\", exp_name)\ncheckpoints_dir = os.path.join(\"checkpoints\", exp_name)\nos.makedirs(logs_dir, exist_ok=True)\nos.makedirs(checkpoints_dir, exist_ok=True)\n\n# Setup logging\nlog_file = os.path.join(logs_dir, \"training.log\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\n\n#best_model_path = os.path.join(checkpoints_dir, \"best_model.pth\")\nbest_model_path = '/kaggle/working/best_model.pth'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:36:10.130947Z","iopub.execute_input":"2025-05-29T12:36:10.131197Z","iopub.status.idle":"2025-05-29T12:36:10.414375Z","shell.execute_reply.started":"2025-05-29T12:36:10.131169Z","shell.execute_reply":"2025-05-29T12:36:10.413632Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nMODEL SETUP\n========================================\nUsing Generalized Cross Entropy (GCE) Loss (q=0.5)\nModel parameters: 1,330,953\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 8. Main training loop","metadata":{}},{"cell_type":"markdown","source":"### 8.1 Learning rate settings","metadata":{}},{"cell_type":"code","source":"# Learning Rate Scheduler Setup\nprint(\"\\n\" + \"=\"*40)\nprint(\"SCHEDULER SETUP\")\nprint(\"=\"*40)\n\n# Update optimizer with initial learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=args.initial_lr)\n\nscheduler = None\nif args.use_scheduler:\n    if args.scheduler_type == 'StepLR':\n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, \n            step_size=args.step_size, \n            gamma=args.gamma\n        )\n        print(f\"Using StepLR scheduler: step_size={args.step_size}, gamma={args.gamma}\")\n        \n    elif args.scheduler_type == 'ReduceLROnPlateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='min',  # We want to reduce LR when validation loss stops decreasing\n            factor=args.factor,\n            patience=args.patience_lr,\n            min_lr=args.min_lr,\n        )\n        print(f\"Using ReduceLROnPlateau scheduler: factor={args.factor}, patience={args.patience_lr}, min_lr={args.min_lr}\")\n        \n    elif args.scheduler_type == 'CosineAnnealingLR':\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=args.T_max,\n            eta_min=args.eta_min\n        )\n        print(f\"Using CosineAnnealingLR scheduler: T_max={args.T_max}, eta_min={args.eta_min}\")\n        \n    elif args.scheduler_type == 'ExponentialLR':\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n            optimizer,\n            gamma=args.gamma_exp\n        )\n        print(f\"Using ExponentialLR scheduler: gamma={args.gamma_exp}\")\n        \n    elif args.scheduler_type == 'OneCycleLR':\n        # Calculate total steps for OneCycleLR\n        total_steps = len(train_loader) * args.epochs\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=args.max_lr,\n            total_steps=total_steps,\n            pct_start=args.pct_start\n        )\n        print(f\"Using OneCycleLR scheduler: max_lr={args.max_lr}, total_steps={total_steps}, pct_start={args.pct_start}\")\n        \n    else:\n        print(f\"Unknown scheduler type: {args.scheduler_type}. No scheduler will be used.\")\n        args.use_scheduler = False\nelse:\n    print(\"No learning rate scheduler will be used.\")\n\nprint(f\"Initial learning rate: {args.initial_lr}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:36:10.415183Z","iopub.execute_input":"2025-05-29T12:36:10.415549Z","iopub.status.idle":"2025-05-29T12:36:10.423531Z","shell.execute_reply.started":"2025-05-29T12:36:10.415522Z","shell.execute_reply":"2025-05-29T12:36:10.422826Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nSCHEDULER SETUP\n========================================\nUsing ReduceLROnPlateau scheduler: factor=0.5, patience=10, min_lr=1e-07\nInitial learning rate: 0.001\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Training loop Call\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"TRAINING\")\nprint(\"=\"*40)\n\nbest_val_accuracy = 0.0\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\nlearning_rates = []  # Track learning rates\n\n# Early stopping variables\nif args.early_stopping:\n    epochs_without_improvement = 0\n    print(f\"Early stopping enabled with patience: {args.patience}\")\nelse:\n    print(\"Early stopping disabled\")\n\n# Calculate checkpoint intervals\nif args.num_checkpoints > 1:\n    checkpoint_intervals = [int((i + 1) * args.epochs / args.num_checkpoints) \n                          for i in range(args.num_checkpoints)]\nelse:\n    checkpoint_intervals = [args.epochs]\n\nfor epoch in range(args.epochs):\n    print(f\"\\nEpoch {epoch + 1}/{args.epochs}\")\n    print(\"-\" * 30)\n\n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    learning_rates.append(current_lr)\n    \n    # Training\n    train_loss, train_acc = train(\n        train_loader, model, optimizer, criterion, device,\n        save_checkpoints=(epoch + 1 in checkpoint_intervals),\n        checkpoint_path=os.path.join(checkpoints_dir, \"checkpoint\"),\n        current_epoch=epoch,\n        scheduler=scheduler,\n        args=args\n    )\n    \n    # Validation\n    val_loss, val_acc = evaluate(val_loader, model, criterion, device, calculate_accuracy=True)\n    \n    # Log results\n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    print(f\"Learning Rate: {current_lr:.2e}\")\n    \n    logging.info(f\"Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n                f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, LR={current_lr:.2e}\")\n    \n    # Store metrics\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    # Save best model\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"★ New best model saved! Val Acc: {val_acc:.4f}\")\n\n        # Reset early stopping counter\n        if args.early_stopping:\n            epochs_without_improvement = 0\n\n    else:\n        # No improvement\n        if args.early_stopping:\n            epochs_without_improvement += 1\n            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n            \n            # Check if we should stop early\n            if epochs_without_improvement >= args.patience:\n                print(f\"\\nEarly stopping triggered! No improvement for {args.patience} epochs.\")\n                print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n                break\n\n    # Learning rate scheduler step\n    if scheduler is not None:\n        if args.scheduler_type == 'ReduceLROnPlateau':\n            # ReduceLROnPlateau needs the metric to monitor\n            scheduler.step(val_loss)\n        elif args.scheduler_type == 'OneCycleLR':\n            # OneCycleLR steps every batch, not every epoch\n            # This is handled in the training function\n            pass\n        else:\n            # Other schedulers step every epoch\n            scheduler.step()\n        \n        # Check if learning rate changed\n        new_lr = optimizer.param_groups[0]['lr']\n        if new_lr != current_lr:\n            print(f\"Learning rate changed: {current_lr:.2e} → {new_lr:.2e}\")\n\nprint(f\"\\nBest validation accuracy: {best_val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T12:36:10.424505Z","iopub.execute_input":"2025-05-29T12:36:10.424776Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nTRAINING\n========================================\nEarly stopping enabled with patience: 25\n\nEpoch 1/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.34batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:00<00:00, 18.29batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0736, Train Acc: 0.3378\nVal Loss: 1.1378, Val Acc: 0.3023\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.3023\n\nEpoch 2/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:16<00:00,  9.92batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:00<00:00, 18.19batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0046, Train Acc: 0.3784\nVal Loss: 0.9918, Val Acc: 0.3945\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.3945\n\nEpoch 3/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:16<00:00,  9.80batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.90batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9753, Train Acc: 0.4015\nVal Loss: 1.0126, Val Acc: 0.3652\nLearning Rate: 1.00e-03\nNo improvement for 1 epoch(s)\n\nEpoch 4/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:16<00:00,  9.60batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.64batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9452, Train Acc: 0.4249\nVal Loss: 1.1261, Val Acc: 0.3076\nLearning Rate: 1.00e-03\nNo improvement for 2 epoch(s)\n\nEpoch 5/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:16<00:00,  9.49batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.42batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9130, Train Acc: 0.4516\nVal Loss: 1.4353, Val Acc: 0.2207\nLearning Rate: 1.00e-03\nNo improvement for 3 epoch(s)\n\nEpoch 6/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.32batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.16batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8954, Train Acc: 0.4613\nVal Loss: 0.9273, Val Acc: 0.4397\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.4397\n\nEpoch 7/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.10batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.91batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8794, Train Acc: 0.4722\nVal Loss: 1.0472, Val Acc: 0.3493\nLearning Rate: 1.00e-03\nNo improvement for 1 epoch(s)\n\nEpoch 8/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  8.93batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8627, Train Acc: 0.4830\nVal Loss: 1.0004, Val Acc: 0.3803\nLearning Rate: 1.00e-03\nNo improvement for 2 epoch(s)\n\nEpoch 9/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.05batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.00batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8434, Train Acc: 0.4985\nVal Loss: 1.0120, Val Acc: 0.3537\nLearning Rate: 1.00e-03\nNo improvement for 3 epoch(s)\n\nEpoch 10/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.14batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.13batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8336, Train Acc: 0.5048\nVal Loss: 0.9455, Val Acc: 0.4344\nLearning Rate: 1.00e-03\nNo improvement for 4 epoch(s)\n\nEpoch 11/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.12batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.95batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8248, Train Acc: 0.5142\nVal Loss: 1.0337, Val Acc: 0.3546\nLearning Rate: 1.00e-03\nNo improvement for 5 epoch(s)\n\nEpoch 12/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.03batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.89batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8109, Train Acc: 0.5233\nVal Loss: 0.8922, Val Acc: 0.4548\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.4548\n\nEpoch 13/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.04batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.85batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7911, Train Acc: 0.5361\nVal Loss: 1.0909, Val Acc: 0.3440\nLearning Rate: 1.00e-03\nNo improvement for 1 epoch(s)\n\nEpoch 14/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.06batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.99batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7837, Train Acc: 0.5423\nVal Loss: 0.8948, Val Acc: 0.4619\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.4619\n\nEpoch 15/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.08batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.99batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7720, Train Acc: 0.5499\nVal Loss: 0.8687, Val Acc: 0.5035\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.5035\n\nEpoch 16/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.10batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.02batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7581, Train Acc: 0.5594\nVal Loss: 0.8696, Val Acc: 0.4690\nLearning Rate: 1.00e-03\nNo improvement for 1 epoch(s)\n\nEpoch 17/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.09batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.96batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7494, Train Acc: 0.5628\nVal Loss: 0.8389, Val Acc: 0.4894\nLearning Rate: 1.00e-03\nNo improvement for 2 epoch(s)\n\nEpoch 18/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.11batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.00batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7366, Train Acc: 0.5707\nVal Loss: 0.9218, Val Acc: 0.4441\nLearning Rate: 1.00e-03\nNo improvement for 3 epoch(s)\n\nEpoch 19/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.07batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 17.00batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7289, Train Acc: 0.5785\nVal Loss: 0.9062, Val Acc: 0.4388\nLearning Rate: 1.00e-03\nNo improvement for 4 epoch(s)\n\nEpoch 20/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.07batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.87batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7227, Train Acc: 0.5868\nVal Loss: 0.7780, Val Acc: 0.5443\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.5443\n\nEpoch 21/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.06batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.97batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7155, Train Acc: 0.5919\nVal Loss: 0.8720, Val Acc: 0.4681\nLearning Rate: 1.00e-03\nNo improvement for 1 epoch(s)\n\nEpoch 22/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs: 100%|██████████| 159/159 [00:17<00:00,  9.06batch/s]\nIterating eval graphs: 100%|██████████| 18/18 [00:01<00:00, 16.91batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7060, Train Acc: 0.5932\nVal Loss: 0.7309, Val Acc: 0.5771\nLearning Rate: 1.00e-03\n★ New best model saved! Val Acc: 0.5771\n\nEpoch 23/200\n------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Iterating training graphs:  36%|███▋      | 58/159 [00:06<00:11,  9.01batch/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Plot training progress\nplot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, learning_rates, logs_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.5 Testing and predictions","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"TESTING\")\nprint(\"=\"*40)\n\n# Load best model and make predictions\nmodel.load_state_dict(torch.load(best_model_path))\nprint(f\"Loaded best model from: {best_model_path}\")\n\npredictions = evaluate(test_loader, model, criterion, device, calculate_accuracy=False)\n\n# Save predictions\nsave_predictions(predictions, args.dataset)\n\n# Cleanup for memory\ndel train_dataset, val_dataset, test_dataset\ndel train_loader, val_loader, test_loader\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*60)\nprint(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\nprint(f\"Predictions saved for dataset {args.dataset}\")\nprint(f\"Logs and plots saved in: {logs_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}