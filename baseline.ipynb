{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7295ee70",
   "metadata": {},
   "source": [
    "# Graph Neural Network Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d423f8",
   "metadata": {},
   "source": [
    "Multi-Dataset Graph Classification with Noise-Robust Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8955b",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228bc1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:24.309534Z",
     "iopub.status.busy": "2025-05-27T12:44:24.308859Z",
     "iopub.status.idle": "2025-05-27T12:44:29.569264Z",
     "shell.execute_reply": "2025-05-27T12:44:29.568217Z",
     "shell.execute_reply.started": "2025-05-27T12:44:24.309508Z"
    },
    "id": "xSkgt1zf-raF",
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "papermill": {
     "duration": 5.620104,
     "end_time": "2025-05-21T16:23:24.361690",
     "exception": false,
     "start_time": "2025-05-21T16:23:18.741586",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b1078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:29.571321Z",
     "iopub.status.busy": "2025-05-27T12:44:29.571048Z",
     "iopub.status.idle": "2025-05-27T12:44:39.642802Z",
     "shell.execute_reply": "2025-05-27T12:44:39.642196Z",
     "shell.execute_reply.started": "2025-05-27T12:44:29.571296Z"
    },
    "id": "lAQuCuIoBbq5",
    "papermill": {
     "duration": 9.949638,
     "end_time": "2025-05-21T16:25:02.510764",
     "exception": false,
     "start_time": "2025-05-21T16:24:52.561126",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bedc6f-2af6-428f-a7db-c00a24038b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:39.644472Z",
     "iopub.status.busy": "2025-05-27T12:44:39.643956Z",
     "iopub.status.idle": "2025-05-27T12:44:40.420528Z",
     "shell.execute_reply": "2025-05-27T12:44:40.419830Z",
     "shell.execute_reply.started": "2025-05-27T12:44:39.644443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "helper_scripts_path = '/kaggle/input/myhackatonhelperscripts/'\n",
    "\n",
    "if os.path.exists(helper_scripts_path):\n",
    "    # Add this path to the beginning of Python's search list\n",
    "    sys.path.insert(0, helper_scripts_path)\n",
    "    print(f\"Successfully added '{helper_scripts_path}' to sys.path.\")\n",
    "    print(f\"Contents of '{helper_scripts_path}': {os.listdir(helper_scripts_path)}\") # Verify\n",
    "else:\n",
    "    print(f\"WARNING: Helper scripts path not found: {helper_scripts_path}\")\n",
    "    print(\"Please ensure 'myhackathonhelperscripts' dataset is correctly added to the notebook.\")\n",
    "\n",
    "# Start import of utils modules\n",
    "try:\n",
    "    from preprocessor import MultiDatasetLoader\n",
    "    from utils import set_seed\n",
    "    # from conv import GINConv as OriginalRepoGINConv\n",
    "    from models import GNN\n",
    "    print(\"Successfully imported modules.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing module: {e}\")\n",
    "    print(\"Please check that the .py files exist directly under the helper_scripts_path and have no syntax errors.\")\n",
    "    # print(\"Current sys.path:\", sys.path)\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544afa5",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c70d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.422450Z",
     "iopub.status.busy": "2025-05-27T12:44:40.422050Z",
     "iopub.status.idle": "2025-05-27T12:44:40.426449Z",
     "shell.execute_reply": "2025-05-27T12:44:40.425642Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.422430Z"
    },
    "id": "Dyf0I2-t9IcW",
    "papermill": {
     "duration": 0.019268,
     "end_time": "2025-05-21T16:25:02.544583",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.525315",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0944127",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622cfa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.427516Z",
     "iopub.status.busy": "2025-05-27T12:44:40.427247Z",
     "iopub.status.idle": "2025-05-27T12:44:40.439727Z",
     "shell.execute_reply": "2025-05-27T12:44:40.438926Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.427491Z"
    },
    "id": "3jKvoQYI9Zbc",
    "papermill": {
     "duration": 0.019599,
     "end_time": "2025-05-21T16:25:02.577661",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.558062",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            output = model(data)\n",
    "        except IndexError as e:\n",
    "            print(f\"Error in batch with {data.num_nodes} nodes, edge_max={data.edge_index.max()}\")\n",
    "            print(f\"Batch info: x.shape={data.x.shape}, edge_index.shape={data.edge_index.shape}\")\n",
    "            raise e\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139b912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.440826Z",
     "iopub.status.busy": "2025-05-27T12:44:40.440560Z",
     "iopub.status.idle": "2025-05-27T12:44:40.450582Z",
     "shell.execute_reply": "2025-05-27T12:44:40.449945Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.440811Z"
    },
    "id": "8peFiIS19ZpK",
    "papermill": {
     "duration": 0.017908,
     "end_time": "2025-05-21T16:25:02.607848",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.589940",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                total_loss += criterion(output, data.y).item()\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader),accuracy\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bba939",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbd871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.452025Z",
     "iopub.status.busy": "2025-05-27T12:44:40.451747Z",
     "iopub.status.idle": "2025-05-27T12:44:40.462447Z",
     "shell.execute_reply": "2025-05-27T12:44:40.461838Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.452007Z"
    },
    "id": "WanuZKxy9Zs-",
    "papermill": {
     "duration": 0.016728,
     "end_time": "2025-05-21T16:25:02.635694",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.618966",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    script_dir = os.getcwd() \n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d24da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.463361Z",
     "iopub.status.busy": "2025-05-27T12:44:40.463121Z",
     "iopub.status.idle": "2025-05-27T12:44:40.475283Z",
     "shell.execute_reply": "2025-05-27T12:44:40.474412Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.463344Z"
    },
    "id": "uyHIJS5U9ZzB",
    "papermill": {
     "duration": 0.017765,
     "end_time": "2025-05-21T16:25:02.664538",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.646773",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n",
    "    \"\"\"\n",
    "    Plot training and validation progress over epochs.\n",
    "    \n",
    "    Args:\n",
    "        train_losses: List of training losses per epoch\n",
    "        train_accuracies: List of training accuracies per epoch  \n",
    "        val_losses: List of validation losses per epoch\n",
    "        val_accuracies: List of validation accuracies per epoch\n",
    "        output_dir: Directory to save the plot\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue', marker='o')\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", color='red', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green', marker='o')\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color='orange', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864fa6c",
   "metadata": {},
   "source": [
    "## 5. Configuration and Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e88b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.476205Z",
     "iopub.status.busy": "2025-05-27T12:44:40.475963Z",
     "iopub.status.idle": "2025-05-27T12:44:40.488009Z",
     "shell.execute_reply": "2025-05-27T12:44:40.487351Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.476157Z"
    },
    "papermill": {
     "duration": 0.017703,
     "end_time": "2025-05-21T16:25:02.721184",
     "exception": false,
     "start_time": "2025-05-21T16:25:02.703481",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    \"\"\"Set training configuration directly\"\"\"\n",
    "    args = {\n",
    "        # Dataset selection\n",
    "        'dataset': 'A',  # Choose: A, B, C, D\n",
    "        'train_mode': 1,  # 1=single dataset, 2=all datasets\n",
    "        \n",
    "        # Model config\n",
    "        'gnn': 'gin',  # gin, gin-virtual, gcn, gcn-virtual\n",
    "        'drop_ratio': 0.0,\n",
    "        'num_layer': 5,\n",
    "        'emb_dim': 300,\n",
    "        \n",
    "        # Training config\n",
    "        'batch_size': 32,\n",
    "        'epochs': 10,\n",
    "        'baseline_mode': 1,  # 1=CE, 2=Noisy CE\n",
    "        'noise_prob': 0.2,\n",
    "        \n",
    "        # System config\n",
    "        'device': 0,\n",
    "        'num_checkpoints': 3\n",
    "    }\n",
    "    return argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bffa19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.490538Z",
     "iopub.status.busy": "2025-05-27T12:44:40.490341Z",
     "iopub.status.idle": "2025-05-27T12:44:40.503968Z",
     "shell.execute_reply": "2025-05-27T12:44:40.503422Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.490523Z"
    },
    "papermill": {
     "duration": 0.118164,
     "end_time": "2025-05-21T16:25:02.850799",
     "exception": true,
     "start_time": "2025-05-21T16:25:02.732635",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def populate_args(args):\n",
    "    print(\"Arguments received:\")\n",
    "    for key, value in vars(args).items():\n",
    "        print(f\"{key}: {value}\")\n",
    "args = get_arguments()\n",
    "populate_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40170fe",
   "metadata": {},
   "source": [
    "## 6. Loss Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b57d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.504884Z",
     "iopub.status.busy": "2025-05-27T12:44:40.504648Z",
     "iopub.status.idle": "2025-05-27T12:44:40.517333Z",
     "shell.execute_reply": "2025-05-27T12:44:40.516509Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.504869Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, p_noisy):\n",
    "        super().__init__()\n",
    "        self.p = p_noisy\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        losses = self.ce(logits, targets)\n",
    "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
    "        return (losses * weights).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319a162",
   "metadata": {},
   "source": [
    "## 7. Main training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210abf8",
   "metadata": {},
   "source": [
    "### 7.1 Config section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a82d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.518150Z",
     "iopub.status.busy": "2025-05-27T12:44:40.517959Z",
     "iopub.status.idle": "2025-05-27T12:44:40.529596Z",
     "shell.execute_reply": "2025-05-27T12:44:40.528974Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.518136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Enhanced GNN Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get configuration\n",
    "args = get_arguments()\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "for key, value in vars(args).items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26506118",
   "metadata": {},
   "source": [
    "### 7.2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81432ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:44:40.530672Z",
     "iopub.status.busy": "2025-05-27T12:44:40.530366Z",
     "iopub.status.idle": "2025-05-27T12:45:01.006287Z",
     "shell.execute_reply": "2025-05-27T12:45:01.005416Z",
     "shell.execute_reply.started": "2025-05-27T12:44:40.530656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "base_path = '/kaggle/input/deep-dataset-preprocessed/processed_data_separate'\n",
    "\n",
    "# Prepare training/validation data based on mode\n",
    "if args.train_mode == 1:\n",
    "    # Single dataset mode\n",
    "    dataset_name = args.dataset\n",
    "    train_dataset = torch.load(f'{base_path}/{dataset_name}_train_graphs.pt', weights_only=False)\n",
    "    train_dataset = [add_zeros(data) for data in train_dataset]\n",
    "    \n",
    "    val_dataset = torch.load(f'{base_path}/{dataset_name}_val_graphs.pt', weights_only=False)\n",
    "    val_dataset = [add_zeros(data) for data in val_dataset]\n",
    "    \n",
    "    test_dataset = torch.load(f'{base_path}/{dataset_name}_test_graphs.pt', weights_only=False)\n",
    "    test_dataset = [add_zeros(data) for data in test_dataset]\n",
    "    print(f\"Using single dataset: {dataset_name}\")\n",
    "else:\n",
    "    # All datasets mode\n",
    "    train_dataset = []\n",
    "    val_dataset = []\n",
    "    test_dataset = torch.load(f'{base_path}/{args.dataset}_test_graphs.pt', weights_only=False)  # Test on specified dataset\n",
    "    \n",
    "    for ds_name in ['A', 'B', 'C', 'D']:\n",
    "        train_dataset.extend(torch.load(f'{base_path}/{ds_name}_train_graphs.pt', weights_only=False))\n",
    "        val_dataset.extend(torch.load(f'{base_path}/{ds_name}_val_graphs.pt', weights_only=False))\n",
    "    \n",
    "    print(\"Using all datasets for training\")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63613e69",
   "metadata": {},
   "source": [
    "### 7.3 Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b5bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:45:01.007395Z",
     "iopub.status.busy": "2025-05-27T12:45:01.007103Z",
     "iopub.status.idle": "2025-05-27T12:45:01.297563Z",
     "shell.execute_reply": "2025-05-27T12:45:01.296871Z",
     "shell.execute_reply.started": "2025-05-27T12:45:01.007378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL SETUP\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Initialize model\n",
    "if args.gnn == 'gin':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, \n",
    "               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False)\n",
    "elif args.gnn == 'gin-virtual':\n",
    "    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer,\n",
    "               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True)\n",
    "elif args.gnn == 'gcn':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer,\n",
    "               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False)\n",
    "elif args.gnn == 'gcn-virtual':\n",
    "    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer,\n",
    "               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True)\n",
    "else:\n",
    "    raise ValueError(f'Invalid GNN type: {args.gnn}')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "if args.baseline_mode == 2:\n",
    "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
    "    print(f\"Using Noisy Cross Entropy Loss (p={args.noise_prob})\")\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    print(\"Using standard Cross Entropy Loss\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Setup logging and checkpoints\n",
    "exp_name = f\"{args.gnn}_dataset{args.dataset}_mode{args.train_mode}\"\n",
    "logs_dir = os.path.join(\"logs\", exp_name)\n",
    "checkpoints_dir = os.path.join(\"checkpoints\", exp_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "log_file = os.path.join(logs_dir, \"training.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_model_path = os.path.join(checkpoints_dir, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25032fa3",
   "metadata": {},
   "source": [
    "### 7.4 Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e3701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:45:01.298867Z",
     "iopub.status.busy": "2025-05-27T12:45:01.298613Z",
     "iopub.status.idle": "2025-05-27T12:51:51.155829Z",
     "shell.execute_reply": "2025-05-27T12:51:51.155081Z",
     "shell.execute_reply.started": "2025-05-27T12:45:01.298841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Calculate checkpoint intervals\n",
    "if args.num_checkpoints > 1:\n",
    "    checkpoint_intervals = [int((i + 1) * args.epochs / args.num_checkpoints) \n",
    "                          for i in range(args.num_checkpoints)]\n",
    "else:\n",
    "    checkpoint_intervals = [args.epochs]\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{args.epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train(\n",
    "        train_loader, model, optimizer, criterion, device,\n",
    "        save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "        checkpoint_path=os.path.join(checkpoints_dir, \"checkpoint\"),\n",
    "        current_epoch=epoch\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
    "    \n",
    "    # Log results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    logging.info(f\"Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "                f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"â˜… New best model saved! Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994101a-2113-4a64-bbd6-17acd9f5988a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:51:51.157064Z",
     "iopub.status.busy": "2025-05-27T12:51:51.156747Z",
     "iopub.status.idle": "2025-05-27T12:51:51.916508Z",
     "shell.execute_reply": "2025-05-27T12:51:51.915737Z",
     "shell.execute_reply.started": "2025-05-27T12:51:51.157037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25604a74",
   "metadata": {},
   "source": [
    "### 7.5 Testing and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27e926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:51:51.917877Z",
     "iopub.status.busy": "2025-05-27T12:51:51.917572Z",
     "iopub.status.idle": "2025-05-27T12:51:56.765462Z",
     "shell.execute_reply": "2025-05-27T12:51:56.764785Z",
     "shell.execute_reply.started": "2025-05-27T12:51:51.917854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"TESTING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load best model and make predictions\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(f\"Loaded best model from: {best_model_path}\")\n",
    "\n",
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
    "\n",
    "# Save predictions\n",
    "save_predictions(predictions, args.dataset)\n",
    "\n",
    "# Cleanup for memory\n",
    "del train_dataset, val_dataset, test_dataset\n",
    "del train_loader, val_loader, test_loader\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Predictions saved for dataset {args.dataset}\")\n",
    "print(f\"Logs and plots saved in: {logs_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7519186,
     "sourceId": 11958675,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7519473,
     "sourceId": 11971157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.457758,
   "end_time": "2025-05-21T16:25:04.482169",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T16:23:16.024411",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
