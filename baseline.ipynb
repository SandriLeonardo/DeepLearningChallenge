{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11934327,"sourceType":"datasetVersion","datasetId":7503117},{"sourceId":11937135,"sourceType":"datasetVersion","datasetId":7504895}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric\nimport sys\nimport os\nimport argparse # We'll use its Namespace for config, not full parsing in notebook\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\n\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GINConv as PyG_GINConv, global_add_pool\n\n\nhelper_scripts_path = '/kaggle/input/myhackathonhelperscripts/'\n\nif os.path.exists(helper_scripts_path):\n    # Add this path to the beginning of Python's search list\n    sys.path.insert(0, helper_scripts_path)\n    print(f\"Successfully added '{helper_scripts_path}' to sys.path.\")\n    print(f\"Contents of '{helper_scripts_path}': {os.listdir(helper_scripts_path)}\") # Verify\nelse:\n    print(f\"WARNING: Helper scripts path not found: {helper_scripts_path}\")\n    print(\"Please ensure 'myhackathonhelperscripts' dataset is correctly added to the notebook.\")\n\n# --- Now you can import your modules ---\ntry:\n    from loadData import GraphDataset # loadData.py is directly in helper_scripts_path\n    from utils import set_seed      # utils.py is directly in helper_scripts_path\n    # from conv import GINConv as OriginalRepoGINConv # If you needed it\n    # from models import GNN as OriginalRepoGNN       # If you needed it\n    print(\"Successfully imported GraphDataset and set_seed.\")\nexcept ImportError as e:\n    print(f\"ERROR importing module: {e}\")\n    print(\"Please check that the .py files exist directly under the helper_scripts_path and have no syntax errors.\")\n    # As a fallback or for debugging, you can try to print sys.path to see where Python is looking\n    # print(\"Current sys.path:\", sys.path)\n\n\n# Call set_seed early\nset_seed(42) # You can change ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:18:00.108806Z","iopub.execute_input":"2025-05-24T20:18:00.109141Z","iopub.status.idle":"2025-05-24T20:18:14.123665Z","shell.execute_reply.started":"2025-05-24T20:18:00.109122Z","shell.execute_reply":"2025-05-24T20:18:14.122815Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\nSuccessfully added '/kaggle/input/myhackathonhelperscripts/' to sys.path.\nContents of '/kaggle/input/myhackathonhelperscripts/': ['loadData.py', 'utils.py', 'models.py', 'conv.py']\nSuccessfully imported GraphDataset and set_seed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def add_zeros(data):\n    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:18:14.124520Z","iopub.execute_input":"2025-05-24T20:18:14.124989Z","iopub.status.idle":"2025-05-24T20:18:14.128593Z","shell.execute_reply.started":"2025-05-24T20:18:14.124954Z","shell.execute_reply":"2025-05-24T20:18:14.127960Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:18:14.130792Z","iopub.execute_input":"2025-05-24T20:18:14.131337Z","iopub.status.idle":"2025-05-24T20:19:30.361936Z","shell.execute_reply.started":"2025-05-24T20:18:14.131311Z","shell.execute_reply":"2025-05-24T20:19:30.361026Z"}},"outputs":[{"name":"stdout","text":"Retrieving folder contents\nRetrieving folder 1wcUVBNQkZ04zStXkglXSgERfIvjSHJiL A\nProcessing file 1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78 test.json.gz\nProcessing file 12N11n8gufNA_C1ns-1IeBseBHgrSfRI1 train.json.gz\nRetrieving folder 1Tj5YoYYDDXjDxxi-cywZgoDkT0b1Qbz- B\nProcessing file 11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww test.json.gz\nProcessing file 13vp-Kwef3UgAwMG-dokGwKyARym9iqtL train.json.gz\nRetrieving folder 1e3B_tBMd693Iwv8x3zRR9c47l5yt_5ey C\nProcessing file 18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT test.json.gz\nProcessing file 1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj train.json.gz\nRetrieving folder 1cvM0eZwpD4gzjo44_zdodxudVBMrLza1 D\nProcessing file 1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u test.json.gz\nProcessing file 1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek train.json.gz\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78\nFrom (redirected): https://drive.google.com/uc?id=1C8sjkO6JS0j2SyVwQ07m8PhQ-pHpuI78&confirm=t&uuid=1c24e4d9-ffb0-4961-adb4-e23c6b5caf4a\nTo: /kaggle/working/datasets/A/test.json.gz\n100%|███████████████████████████████████████| 92.4M/92.4M [00:00<00:00, 187MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1\nFrom (redirected): https://drive.google.com/uc?id=12N11n8gufNA_C1ns-1IeBseBHgrSfRI1&confirm=t&uuid=db93fdec-3af9-464d-8f59-009ac5e9b4af\nTo: /kaggle/working/datasets/A/train.json.gz\n100%|█████████████████████████████████████████| 465M/465M [00:02<00:00, 215MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww\nFrom (redirected): https://drive.google.com/uc?id=11GBlrXMdP3HSD60w-56Tu6rbGkR-Ifww&confirm=t&uuid=8b6384d6-7763-4e35-af9f-df2aad8a450f\nTo: /kaggle/working/datasets/B/test.json.gz\n100%|███████████████████████████████████████| 63.0M/63.0M [00:00<00:00, 268MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL\nFrom (redirected): https://drive.google.com/uc?id=13vp-Kwef3UgAwMG-dokGwKyARym9iqtL&confirm=t&uuid=6b7c90d2-b5b7-46a4-9a2b-4b9bdffc8246\nTo: /kaggle/working/datasets/B/train.json.gz\n100%|█████████████████████████████████████████| 223M/223M [00:00<00:00, 310MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT\nFrom (redirected): https://drive.google.com/uc?id=18XVe65ZsQ0PDLCqQa4WmneVhyfjGcXmT&confirm=t&uuid=3309edec-003e-434b-826a-71651e286398\nTo: /kaggle/working/datasets/C/test.json.gz\n100%|███████████████████████████████████████| 60.5M/60.5M [00:00<00:00, 232MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj\nFrom (redirected): https://drive.google.com/uc?id=1z5lvG2CytbLQZt7Jmo9BopzFd0pKejEj&confirm=t&uuid=ff6145a5-61b7-42c3-a7ba-b510d3618310\nTo: /kaggle/working/datasets/C/train.json.gz\n100%|█████████████████████████████████████████| 308M/308M [00:00<00:00, 329MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u\nFrom (redirected): https://drive.google.com/uc?id=1Gna_dHnBLX8vKaYGAAqAbw5QPerrNK1u&confirm=t&uuid=b94cb376-a15e-4ed1-8d4e-ab2cc335a0a3\nTo: /kaggle/working/datasets/D/test.json.gz\n100%|███████████████████████████████████████| 94.0M/94.0M [00:00<00:00, 265MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek\nFrom (redirected): https://drive.google.com/uc?id=1Pc-6LMML80-AgEoLVs2Q5hLtmR_rTEek&confirm=t&uuid=cf76a5cd-65c5-47ed-a3fe-946c17dd26dc\nTo: /kaggle/working/datasets/D/train.json.gz\n100%|█████████████████████████████████████████| 439M/439M [00:01<00:00, 223MB/s]\nDownload completed\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class ImprovedGIN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_gnn_layers=3, dropout_rate=0.5):\n        super(ImprovedGIN, self).__init__()\n        # Embedding for nodes if data.x comes from add_zeros (all zeros)\n        # This means we learn a single feature vector for all nodes initially.\n        self.embedding = torch.nn.Embedding(num_embeddings=1, embedding_dim=input_dim)\n        self.dropout_rate = dropout_rate\n        self.num_gnn_layers = num_gnn_layers\n\n        self.convs = torch.nn.ModuleList()\n        self.batch_norms = torch.nn.ModuleList()\n\n        # Initial GIN layer's MLP: maps from input_dim (from embedding) to hidden_dim\n        # PyG_GINConv takes an nn.Sequential (MLP) as its first argument.\n        # The MLP's input dim should be input_dim, output dim can be hidden_dim.\n        mlp_initial = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, hidden_dim) # GIN paper suggests MLP output dim == GINConv output dim\n        )\n        self.convs.append(PyG_GINConv(mlp_initial, train_eps=True)) # train_eps=True is common for GIN\n        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n\n        # Subsequent GIN layers: map from hidden_dim to hidden_dim\n        for _ in range(1, self.num_gnn_layers):\n            mlp = torch.nn.Sequential(\n                torch.nn.Linear(hidden_dim, hidden_dim),\n                torch.nn.ReLU(),\n                torch.nn.Linear(hidden_dim, hidden_dim)\n            )\n            self.convs.append(PyG_GINConv(mlp, train_eps=True))\n            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n\n        self.global_pool = global_add_pool # Using global_add_pool\n\n        # Classifier: A 2-layer MLP\n        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        x = self.embedding(x) # Shape: (num_nodes, input_dim)\n        \n        residual_x = None # To store the output for residual connection\n\n        for i in range(self.num_gnn_layers):\n            x_conv_input = x\n            # For GIN with train_eps=True, the original node features are added internally by the layer\n            # based on (1+eps)*x_orig + aggregated_neighbors.\n            # So the residual connection should ideally be applied after the GINConv operation if mimicking GIN paper's layer.\n            # Or, a simpler skip connection: input to layer i is output of layer i-1.\n            \n            x = self.convs[i](x_conv_input, edge_index)\n            x = self.batch_norms[i](x)\n            \n            # Add residual connection\n            if residual_x is not None and i > 0 : # Add output of previous layer (after activation and dropout)\n                 x = x + residual_x\n            \n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n            residual_x = x # Store for next layer's residual connection\n\n        x_pooled = self.global_pool(x, batch)\n\n        x_pooled = F.dropout(x_pooled, p=self.dropout_rate, training=self.training)\n        x_fc1 = self.fc1(x_pooled)\n        x_relu_fc1 = F.relu(x_fc1)\n        x_dropout_fc1 = F.dropout(x_relu_fc1, p=self.dropout_rate, training=self.training)\n        out = self.fc2(x_dropout_fc1)\n        \n        return out\n\nprint(\"ImprovedGIN model class defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:30.363052Z","iopub.execute_input":"2025-05-24T20:19:30.363663Z","iopub.status.idle":"2025-05-24T20:19:30.376792Z","shell.execute_reply.started":"2025-05-24T20:19:30.363628Z","shell.execute_reply":"2025-05-24T20:19:30.375936Z"}},"outputs":[{"name":"stdout","text":"ImprovedGIN model class defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class ImprovedGIN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_gnn_layers=3, dropout_rate=0.5):\n        super(ImprovedGIN, self).__init__()\n        # Embedding for nodes if data.x comes from add_zeros (all zeros)\n        # This means we learn a single feature vector for all nodes initially.\n        self.embedding = torch.nn.Embedding(num_embeddings=1, embedding_dim=input_dim)\n        self.dropout_rate = dropout_rate\n        self.num_gnn_layers = num_gnn_layers\n\n        self.convs = torch.nn.ModuleList()\n        self.batch_norms = torch.nn.ModuleList()\n\n        # Initial GIN layer's MLP: maps from input_dim (from embedding) to hidden_dim\n        # PyG_GINConv takes an nn.Sequential (MLP) as its first argument.\n        # The MLP's input dim should be input_dim, output dim can be hidden_dim.\n        mlp_initial = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, hidden_dim) # GIN paper suggests MLP output dim == GINConv output dim\n        )\n        self.convs.append(PyG_GINConv(mlp_initial, train_eps=True)) # train_eps=True is common for GIN\n        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n\n        # Subsequent GIN layers: map from hidden_dim to hidden_dim\n        for _ in range(1, self.num_gnn_layers):\n            mlp = torch.nn.Sequential(\n                torch.nn.Linear(hidden_dim, hidden_dim),\n                torch.nn.ReLU(),\n                torch.nn.Linear(hidden_dim, hidden_dim)\n            )\n            self.convs.append(PyG_GINConv(mlp, train_eps=True))\n            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n\n        self.global_pool = global_add_pool # Using global_add_pool\n\n        # Classifier: A 2-layer MLP\n        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        x = self.embedding(x) # Shape: (num_nodes, input_dim)\n        \n        residual_x = None # To store the output for residual connection\n\n        for i in range(self.num_gnn_layers):\n            x_conv_input = x\n            # For GIN with train_eps=True, the original node features are added internally by the layer\n            # based on (1+eps)*x_orig + aggregated_neighbors.\n            # So the residual connection should ideally be applied after the GINConv operation if mimicking GIN paper's layer.\n            # Or, a simpler skip connection: input to layer i is output of layer i-1.\n            \n            x = self.convs[i](x_conv_input, edge_index)\n            x = self.batch_norms[i](x)\n            \n            # Add residual connection\n            if residual_x is not None and i > 0 : # Add output of previous layer (after activation and dropout)\n                 x = x + residual_x\n            \n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n            residual_x = x # Store for next layer's residual connection\n\n        x_pooled = self.global_pool(x, batch)\n\n        x_pooled = F.dropout(x_pooled, p=self.dropout_rate, training=self.training)\n        x_fc1 = self.fc1(x_pooled)\n        x_relu_fc1 = F.relu(x_fc1)\n        x_dropout_fc1 = F.dropout(x_relu_fc1, p=self.dropout_rate, training=self.training)\n        out = self.fc2(x_dropout_fc1)\n        \n        return out\n\nprint(\"ImprovedGIN model class defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:30.377527Z","iopub.execute_input":"2025-05-24T20:19:30.377796Z","iopub.status.idle":"2025-05-24T20:19:32.341768Z","shell.execute_reply.started":"2025-05-24T20:19:30.377773Z","shell.execute_reply":"2025-05-24T20:19:32.340935Z"}},"outputs":[{"name":"stdout","text":"ImprovedGIN model class defined.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def train(data_loader, model, optimizer, criterion, device, \n          save_checkpoints_periodically, periodic_checkpoint_dir_base, \n          dataset_name_for_checkpoint, current_epoch):\n    model.train()\n    total_loss = 0\n    correct_preds = 0\n    total_samples = 0\n\n    for data in tqdm(data_loader, desc=f\"Epoch {current_epoch+1} Training\", unit=\"batch\", leave=False):\n        data = data.to(device)\n        if data.y is None: # Skip if no labels (should not happen in training)\n            continue\n        if data.y.numel() == 0: # Skip if batch is empty or labels are empty\n            continue\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, data.y)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * data.num_graphs # Loss is mean over batch, scale by batch size\n        pred = output.argmax(dim=1)\n        correct_preds += (pred == data.y).sum().item()\n        total_samples += data.y.size(0)\n\n    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n    accuracy = correct_preds / total_samples if total_samples > 0 else 0\n\n    if save_checkpoints_periodically:\n        # e.g. checkpoints/A/model_A_epoch_10.pth\n        final_periodic_checkpoint_dir = os.path.join(periodic_checkpoint_dir_base, dataset_name_for_checkpoint)\n        os.makedirs(final_periodic_checkpoint_dir, exist_ok=True)\n        checkpoint_file = os.path.join(final_periodic_checkpoint_dir, f\"model_{dataset_name_for_checkpoint}_epoch_{current_epoch + 1}.pth\")\n        torch.save(model.state_dict(), checkpoint_file)\n        logging.info(f\"Periodic checkpoint saved: {checkpoint_file}\")\n\n    return avg_loss, accuracy\n\nprint(\"train function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:32.342659Z","iopub.execute_input":"2025-05-24T20:19:32.342956Z","iopub.status.idle":"2025-05-24T20:19:32.360227Z","shell.execute_reply.started":"2025-05-24T20:19:32.342930Z","shell.execute_reply":"2025-05-24T20:19:32.359536Z"}},"outputs":[{"name":"stdout","text":"train function defined.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def evaluate(data_loader, model, device, criterion_for_loss=None, calculate_accuracy_and_loss=False):\n    model.eval()\n    correct_preds = 0\n    total_samples = 0\n    predictions_list = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\", leave=False):\n            data = data.to(device)\n            if data.y is None and calculate_accuracy_and_loss: # Cannot calculate accuracy without labels\n                continue\n            if data.y is not None and data.y.numel() == 0 and calculate_accuracy_and_loss:\n                 continue\n\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if calculate_accuracy_and_loss and data.y is not None:\n                correct_preds += (pred == data.y).sum().item()\n                total_samples += data.y.size(0)\n                if criterion_for_loss:\n                    # Ensure criterion handles cases where output might be for fewer samples than data.y if some were filtered\n                    valid_indices = (data.y != -1) # Example: if -1 indicates no label for a sample\n                    if output.size(0) == data.y[valid_indices].size(0): # Check if output matches filtered labels\n                         total_loss += criterion_for_loss(output, data.y[valid_indices]).item() * output.size(0)\n                    elif output.size(0) == data.y.size(0):\n                         total_loss += criterion_for_loss(output, data.y).item() * data.num_graphs\n\n\n            else: # Only collecting predictions for test set usually\n                predictions_list.extend(pred.cpu().numpy())\n\n    if calculate_accuracy_and_loss:\n        accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n        avg_loss = total_loss / total_samples if total_samples > 0 and criterion_for_loss else 0.0\n        return avg_loss, accuracy\n    return predictions_list\n\nprint(\"evaluate function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:32.360911Z","iopub.execute_input":"2025-05-24T20:19:32.361071Z","iopub.status.idle":"2025-05-24T20:19:32.372875Z","shell.execute_reply.started":"2025-05-24T20:19:32.361058Z","shell.execute_reply":"2025-05-24T20:19:32.372338Z"}},"outputs":[{"name":"stdout","text":"evaluate function defined.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def save_predictions(predictions_to_save, test_path_arg, root_submission_folder=\"submission\", id_col_name=\"id\", pred_col_name=\"pred\"):\n    current_dir = os.getcwd() # Use current_dir for clarity\n    submission_folder_main_path = os.path.join(current_dir, root_submission_folder)\n    os.makedirs(submission_folder_main_path, exist_ok=True)\n\n    dataset_folder_name = \"datasets\"\n    if test_path_arg:\n        try:\n            # Assumes test_path_arg is like './datasets/A/test.json.gz'\n            dataset_folder_name = os.path.basename(os.path.dirname(test_path_arg))\n            if not dataset_folder_name or dataset_folder_name == \".\": # Handle edge cases\n                # if path is just 'test.json.gz' or './test.json.gz'\n                path_parts = os.path.normpath(test_path_arg).split(os.sep)\n                if len(path_parts) > 2 and path_parts[-2] != \"datasets\": # e.g. datasets/A/file\n                    dataset_folder_name = path_parts[-2]\n                elif len(path_parts) > 1 and path_parts[-2] == \"datasets\": # e.g. datasets/file (no subfolder)\n                     dataset_folder_name = \"root_dataset\"\n                else: # single file\n                    dataset_folder_name = \"direct_file_dataset\"\n\n\n        except Exception as e:\n            print(f\"Error parsing dataset name from test_path: {e}, using default.\")\n            \n    output_csv_filename = f\"testset_{dataset_folder_name}.csv\"\n    output_csv_full_path = os.path.join(submission_folder_main_path, output_csv_filename)\n    \n    # Assuming predictions are for sequentially indexed graphs\n    graph_ids = list(range(len(predictions_to_save)))\n    output_df = pd.DataFrame({\n        id_col_name: graph_ids,\n        pred_col_name: predictions_to_save\n    })\n    \n    output_df.to_csv(output_csv_full_path, index=False)\n    logging.info(f\"Predictions saved to {output_csv_full_path}\")\n    print(f\"Predictions saved to {output_csv_full_path}\")\n\nprint(\"save_predictions function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:32.373793Z","iopub.execute_input":"2025-05-24T20:19:32.374036Z","iopub.status.idle":"2025-05-24T20:19:32.392856Z","shell.execute_reply.started":"2025-05-24T20:19:32.374014Z","shell.execute_reply":"2025-05-24T20:19:32.392295Z"}},"outputs":[{"name":"stdout","text":"save_predictions function defined.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def plot_training_progress(losses_data, accuracies_data, plot_title_prefix, output_dir_for_plots, dataset_name_for_plot):\n    epochs_count = range(1, len(losses_data) + 1)\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5)) # Use subplots for clarity\n\n    # Plot Loss\n    axs[0].plot(epochs_count, losses_data, 'b-o', label=f\"{plot_title_prefix} Loss\")\n    axs[0].set_title(f'{plot_title_prefix} Loss ({dataset_name_for_plot})')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n    axs[0].grid(True)\n\n    # Plot Accuracy\n    axs[1].plot(epochs_count, accuracies_data, 'g-o', label=f\"{plot_title_prefix} Accuracy\")\n    axs[1].set_title(f'{plot_title_prefix} Accuracy ({dataset_name_for_plot})')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy')\n    axs[1].legend()\n    axs[1].grid(True)\n\n    os.makedirs(output_dir_for_plots, exist_ok=True)\n    plot_filename = f\"{plot_title_prefix.lower().replace(' ', '_')}_{dataset_name_for_plot}_progress.png\"\n    fig.savefig(os.path.join(output_dir_for_plots, plot_filename))\n    plt.show() # Show plot in notebook\n    plt.close(fig) # Close the figure to free memory\n    logging.info(f\"Plot saved: {os.path.join(output_dir_for_plots, plot_filename)}\")\n\nprint(\"plot_training_progress function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:32.394979Z","iopub.execute_input":"2025-05-24T20:19:32.395280Z","iopub.status.idle":"2025-05-24T20:19:32.409444Z","shell.execute_reply.started":"2025-05-24T20:19:32.395263Z","shell.execute_reply":"2025-05-24T20:19:32.408824Z"}},"outputs":[{"name":"stdout","text":"plot_training_progress function defined.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class ExperimentConfig:\n    def __init__(self):\n        # --- Crucial: Dataset Paths ---\n        # Ensure these paths are correct for your setup.\n        # Example: To run for dataset 'A'\n        self.dataset_letter = 'A' # CHANGE THIS FOR DIFFERENT DATASETS (A, B, C, D)\n        \n        # Base path to where 'A', 'B', etc. folders are located\n        self.base_dataset_path = \"/kaggle/working/datasets\" # Assumes 'datasets/A', 'datasets/B', etc.\n        self.train_path = os.path.join(self.base_dataset_path, self.dataset_letter, \"train.json.gz\")\n        self.test_path = os.path.join(self.base_dataset_path, self.dataset_letter, \"test.json.gz\")\n        # Set self.train_path = None to skip training and only test (requires a best_model checkpoint)\n        # self.train_path = None \n\n        # --- Model Hyperparameters (for ImprovedGIN) ---\n        self.input_dim = 300         # For the embedding layer with add_zeros\n        self.hidden_dim = 128        # Hidden dimension for GIN layers\n        self.num_gnn_layers = 3      # Number of GIN layers in ImprovedGIN\n        self.dropout_rate = 0.3      # Dropout rate used in ImprovedGIN\n\n        # --- Training Hyperparameters ---\n        self.epochs = 20             # Number of epochs to train (increased from 10 for better model)\n        self.batch_size = 32\n        self.lr = 0.001              # Learning rate\n\n        # --- Loss Function ---\n        # Options: \"CE\" for CrossEntropyLoss, \"NoisyCE\" for NoisyCrossEntropyLoss\n        self.loss_type = \"CE\"\n        self.num_classes = 6         # Required for NoisyCrossEntropyLoss if used, and model output\n        self.noise_prob = 0.1        # p_noisy for NoisyCrossEntropyLoss (if self.loss_type is \"NoisyCE\")\n\n        # --- Checkpoints & Logging & Submission ---\n        self.num_periodic_checkpoints = 3 # How many checkpoints to save during training epochs\n                                         # (e.g., 3 means save at 1/3, 2/3, and end of epochs)\n                                         # The 'best' model based on validation is always saved.\n        self.base_checkpoint_dir = \"/kaggle/working/checkpoints\"\n        self.base_log_dir = \"/kaggle/working/logs\"\n        self.base_submission_dir = \"/kaggle/working/submission\"\n\n        # --- Device ---\n        self.force_cpu = False # Set to True to force CPU even if CUDA is available\n\n# Instantiate the configuration\nconfig = ExperimentConfig()\n\n# --- Validate paths ---\nif config.train_path and not os.path.exists(config.train_path):\n    print(f\"WARNING: Train path does not exist: {config.train_path}\")\n    # config.train_path = None # Optionally disable training if path missing\nif not os.path.exists(config.test_path):\n    raise FileNotFoundError(f\"CRITICAL: Test path does not exist: {config.test_path}\")\n\nprint(f\"Configuration for Dataset '{config.dataset_letter}':\")\nfor key, value in vars(config).items():\n    print(f\"  {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:32.410231Z","iopub.execute_input":"2025-05-24T20:19:32.410466Z","iopub.status.idle":"2025-05-24T20:19:32.428032Z","shell.execute_reply.started":"2025-05-24T20:19:32.410451Z","shell.execute_reply":"2025-05-24T20:19:32.427491Z"}},"outputs":[{"name":"stdout","text":"Configuration for Dataset 'A':\n  dataset_letter: A\n  base_dataset_path: /kaggle/working/datasets\n  train_path: /kaggle/working/datasets/A/train.json.gz\n  test_path: /kaggle/working/datasets/A/test.json.gz\n  input_dim: 300\n  hidden_dim: 128\n  num_gnn_layers: 3\n  dropout_rate: 0.3\n  epochs: 20\n  batch_size: 32\n  lr: 0.001\n  loss_type: CE\n  num_classes: 6\n  noise_prob: 0.1\n  num_periodic_checkpoints: 3\n  base_checkpoint_dir: /kaggle/working/checkpoints\n  base_log_dir: /kaggle/working/logs\n  base_submission_dir: /kaggle/working/submission\n  force_cpu: False\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ---- 0. Setup Device ----\ndevice = torch.device('cuda' if torch.cuda.is_available() and not config.force_cpu else 'cpu')\nprint(f\"========= Running on Device: {device} =========\")\n\n# ---- 1. Determine Dataset Name for file/folder naming ----\n# This is now handled by config.dataset_letter directly.\ndataset_name_fs = config.dataset_letter # Filesystem-safe dataset name\nprint(f\"========= Processing Dataset: {dataset_name_fs} =========\")\n\n# ---- 2. Setup Output Directories ----\n# Checkpoints: base_checkpoint_dir / dataset_name_fs / model_files... (periodic)\n#            also base_checkpoint_dir / model_DATASET_best.pth (best val)\n# Logs: base_log_dir / dataset_name_fs / logfile.log & plots...\n# Submission: base_submission_dir / testset_DATASET.csv\n\ndataset_periodic_checkpoint_storage_dir = os.path.join(config.base_checkpoint_dir, dataset_name_fs)\nos.makedirs(dataset_periodic_checkpoint_storage_dir, exist_ok=True)\nbest_model_overall_checkpoint_path = os.path.join(config.base_checkpoint_dir, f\"model_{dataset_name_fs}_best.pth\")\n\ndataset_specific_log_dir = os.path.join(config.base_log_dir, dataset_name_fs)\nos.makedirs(dataset_specific_log_dir, exist_ok=True)\nmain_log_file_path = os.path.join(dataset_specific_log_dir, \"experiment.log\")\n\n# ---- 3. Setup Logging ----\n# Remove existing handlers to prevent duplicate logs if re-running cell\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(main_log_file_path, mode='w'), # Overwrite log file each run for this dataset\n        logging.StreamHandler() # Also print to console\n    ]\n)\nlogging.info(f\"Logging setup for dataset {dataset_name_fs}. Log file: {main_log_file_path}\")\nlogging.info(f\"Using device: {device}\")\nfor key, value in vars(config).items(): # Log the config\n    logging.info(f\"CONFIG - {key}: {value}\")\n\n\n# ---- 4. Initialize Model, Optimizer, Criterion ----\nmodel = ImprovedGIN(\n    input_dim=config.input_dim,\n    hidden_dim=config.hidden_dim,\n    output_dim=config.num_classes,\n    num_gnn_layers=config.num_gnn_layers,\n    dropout_rate=config.dropout_rate\n).to(device)\nlogging.info(f\"Model: ImprovedGIN initialized: layers={config.num_gnn_layers}, hidden_dim={config.hidden_dim}, dropout={config.dropout_rate}\")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\nlogging.info(f\"Optimizer: Adam with LR={config.lr}\")\n\nif config.loss_type.upper() == \"NOISYCE\":\n    criterion = NoisyCrossEntropyLoss(p_noisy=config.noise_prob, num_classes=config.num_classes)\n    logging.info(f\"Using NoisyCrossEntropyLoss with p_noisy={config.noise_prob}\")\nelse:\n    criterion = torch.nn.CrossEntropyLoss()\n    logging.info(\"Using standard CrossEntropyLoss\")\n\n# ---- 5. Data Loading ----\nif not os.path.exists(config.test_path):\n    logging.error(f\"Test path {config.test_path} not found. Exiting.\")\n    raise FileNotFoundError(f\"Test path {config.test_path} not found.\")\ntest_dataset = GraphDataset(config.test_path, transform=add_zeros)\ntest_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\nlogging.info(f\"Loaded test data from {config.test_path} ({len(test_dataset)} graphs)\")\n\n# ---- 6. Training Phase ----\nif config.train_path and os.path.exists(config.train_path):\n    logging.info(f\"Loading training data from {config.train_path}\")\n    full_train_dataset_obj = GraphDataset(config.train_path, transform=add_zeros)\n    logging.info(f\"Full training dataset loaded ({len(full_train_dataset_obj)} graphs)\")\n    \n    val_split_ratio = 0.2 # 20% for validation\n    num_total_train_graphs = len(full_train_dataset_obj)\n    num_val_graphs = int(val_split_ratio * num_total_train_graphs)\n    num_train_graphs = num_total_train_graphs - num_val_graphs\n\n    split_generator = torch.Generator().manual_seed(42) # For reproducible splits\n    train_dataset_split_obj, val_dataset_split_obj = torch.utils.data.random_split(\n        full_train_dataset_obj, [num_train_graphs, num_val_graphs], generator=split_generator\n    )\n\n    train_loader = DataLoader(train_dataset_split_obj, batch_size=config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset_split_obj, batch_size=config.batch_size, shuffle=False)\n    logging.info(f\"Training data split: {len(train_dataset_split_obj)} train, {len(val_dataset_split_obj)} val graphs.\")\n\n    best_val_accuracy = -1.0 # Initialize to a low value\n    train_losses_hist, train_accuracies_hist = [], []\n    val_losses_hist, val_accuracies_hist = [], []\n\n    # Determine epochs for periodic checkpoint saving\n    epochs_for_periodic_checkpoints = set()\n    if config.num_periodic_checkpoints > 0 and config.epochs > 0:\n        periodic_interval = config.epochs // config.num_periodic_checkpoints\n        if periodic_interval > 0:\n            for i in range(1, config.num_periodic_checkpoints + 1):\n                epochs_for_periodic_checkpoints.add(i * periodic_interval)\n    epochs_for_periodic_checkpoints.add(config.epochs) # Always save at the final epoch\n\n    logging.info(f\"Starting training for {config.epochs} epochs...\")\n    for epoch in range(config.epochs):\n        save_this_epoch_periodically = (epoch + 1) in epochs_for_periodic_checkpoints\n        \n        avg_epoch_train_loss, epoch_train_acc = train(\n            train_loader, model, optimizer, criterion, device,\n            save_checkpoints_periodically=save_this_epoch_periodically,\n            periodic_checkpoint_dir_base=config.base_checkpoint_dir,\n            dataset_name_for_checkpoint=dataset_name_fs,\n            current_epoch=epoch\n        )\n        \n        avg_epoch_val_loss, epoch_val_acc = evaluate(\n            val_loader, model, device, \n            criterion_for_loss=criterion, calculate_accuracy_and_loss=True\n        )\n\n        logging.info(f\"Epoch {epoch+1}/{config.epochs} -> Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n\n        train_losses_hist.append(avg_epoch_train_loss)\n        train_accuracies_hist.append(epoch_train_acc)\n        val_losses_hist.append(avg_epoch_val_loss)\n        val_accuracies_hist.append(epoch_val_acc)\n\n        if epoch_val_acc > best_val_accuracy:\n            best_val_accuracy = epoch_val_acc\n            torch.save(model.state_dict(), best_model_overall_checkpoint_path)\n            logging.info(f\"*** New best validation accuracy: {best_val_accuracy:.4f}. Best model saved to {best_model_overall_checkpoint_path} ***\")\n    \n    logging.info(\"Training finished.\")\n    # Plotting training and validation progress\n    plots_output_directory = os.path.join(dataset_specific_log_dir, \"plots\")\n    plot_training_progress(train_losses_hist, train_accuracies_hist, \"Training\", plots_output_directory, dataset_name_fs)\n    plot_training_progress(val_losses_hist, val_accuracies_hist, \"Validation\", plots_output_directory, dataset_name_fs)\n    logging.info(f\"Training & validation plots saved to {plots_output_directory}\")\n\n    # Clean up large objects from memory\n    del full_train_dataset_obj, train_dataset_split_obj, val_dataset_split_obj, train_loader, val_loader\n    import gc\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\nelif not config.train_path :\n    logging.info(\"config.train_path is None. Skipping training. Model will be loaded for testing.\")\nelse: # train_path was specified but not found\n     logging.warning(f\"Training path {config.train_path} was specified but not found. Skipping training. Attempting to load best model for testing.\")\n\n\n# ---- 7. Testing Phase ----\nif not os.path.exists(best_model_overall_checkpoint_path):\n    logging.error(f\"Best model checkpoint {best_model_overall_checkpoint_path} not found. Cannot proceed with testing. Ensure training was run or a valid checkpoint exists.\")\n    raise FileNotFoundError(f\"Best model checkpoint {best_model_overall_checkpoint_path} not found.\")\n\nlogging.info(f\"Loading best model for testing from: {best_model_overall_checkpoint_path}\")\ntry:\n    # Ensure model is on the correct device before loading state_dict if it was saved from a different device\n    model.to(device) \n    model.load_state_dict(torch.load(best_model_overall_checkpoint_path, map_location=device))\n    logging.info(\"Best model loaded successfully for testing.\")\nexcept Exception as e:\n    logging.error(f\"Failed to load best model from {best_model_overall_checkpoint_path}: {e}\")\n    raise\n\nfinal_test_predictions = evaluate(test_loader, model, device, calculate_accuracy_and_loss=False)\n\nsave_predictions(final_test_predictions, config.test_path, root_submission_folder=config.base_submission_dir)\nlogging.info(f\"========= Experiment for dataset {dataset_name_fs} finished. =========\")\n\n# Clean up console handler for potential re-runs of the cell\nlogging.getLogger().removeHandler(console_handler) # Assumes console_handler was defined if used","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:19:32.428683Z","iopub.execute_input":"2025-05-24T20:19:32.428871Z"}},"outputs":[{"name":"stderr","text":"2025-05-24 20:19:32,448 - INFO - Logging setup for dataset A. Log file: /kaggle/working/logs/A/experiment.log\n2025-05-24 20:19:32,449 - INFO - Using device: cuda\n2025-05-24 20:19:32,450 - INFO - CONFIG - dataset_letter: A\n2025-05-24 20:19:32,451 - INFO - CONFIG - base_dataset_path: /kaggle/working/datasets\n2025-05-24 20:19:32,451 - INFO - CONFIG - train_path: /kaggle/working/datasets/A/train.json.gz\n2025-05-24 20:19:32,452 - INFO - CONFIG - test_path: /kaggle/working/datasets/A/test.json.gz\n2025-05-24 20:19:32,452 - INFO - CONFIG - input_dim: 300\n2025-05-24 20:19:32,453 - INFO - CONFIG - hidden_dim: 128\n2025-05-24 20:19:32,454 - INFO - CONFIG - num_gnn_layers: 3\n2025-05-24 20:19:32,454 - INFO - CONFIG - dropout_rate: 0.3\n2025-05-24 20:19:32,455 - INFO - CONFIG - epochs: 20\n2025-05-24 20:19:32,457 - INFO - CONFIG - batch_size: 32\n2025-05-24 20:19:32,457 - INFO - CONFIG - lr: 0.001\n2025-05-24 20:19:32,458 - INFO - CONFIG - loss_type: CE\n2025-05-24 20:19:32,458 - INFO - CONFIG - num_classes: 6\n2025-05-24 20:19:32,459 - INFO - CONFIG - noise_prob: 0.1\n2025-05-24 20:19:32,460 - INFO - CONFIG - num_periodic_checkpoints: 3\n2025-05-24 20:19:32,460 - INFO - CONFIG - base_checkpoint_dir: /kaggle/working/checkpoints\n2025-05-24 20:19:32,461 - INFO - CONFIG - base_log_dir: /kaggle/working/logs\n2025-05-24 20:19:32,462 - INFO - CONFIG - base_submission_dir: /kaggle/working/submission\n2025-05-24 20:19:32,462 - INFO - CONFIG - force_cpu: False\n","output_type":"stream"},{"name":"stdout","text":"========= Running on Device: cuda =========\n========= Processing Dataset: A =========\n","output_type":"stream"},{"name":"stderr","text":"2025-05-24 20:19:32,710 - INFO - Model: ImprovedGIN initialized: layers=3, hidden_dim=128, dropout=0.3\n2025-05-24 20:19:32,712 - INFO - Optimizer: Adam with LR=0.001\n2025-05-24 20:19:32,713 - INFO - Using standard CrossEntropyLoss\n2025-05-24 20:20:07,235 - INFO - Loaded test data from /kaggle/working/datasets/A/test.json.gz (2340 graphs)\n2025-05-24 20:20:07,236 - INFO - Loading training data from /kaggle/working/datasets/A/train.json.gz\n","output_type":"stream"}],"execution_count":null}]}