{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11958675,"sourceType":"datasetVersion","datasetId":7519186},{"sourceId":11959204,"sourceType":"datasetVersion","datasetId":7519473}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":108.457758,"end_time":"2025-05-21T16:25:04.482169","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-21T16:23:16.024411","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7295ee70","cell_type":"markdown","source":"# Graph Neural Network Training Pipeline","metadata":{}},{"id":"52d423f8","cell_type":"markdown","source":"Multi-Dataset Graph Classification with Noise-Robust Training","metadata":{}},{"id":"10c8955b","cell_type":"markdown","source":"## 1. Setup and Dependencies","metadata":{}},{"id":"6228bc1c","cell_type":"code","source":"!pip install torch_geometric","metadata":{"id":"xSkgt1zf-raF","outputId":"59f4a52f-5eb4-41e5-9fba-07432989fe78","papermill":{"duration":5.620104,"end_time":"2025-05-21T16:23:24.361690","exception":false,"start_time":"2025-05-21T16:23:18.741586","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"817b1078","cell_type":"code","source":"import os\nimport gc\nimport sys\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import random_split\nimport argparse","metadata":{"id":"lAQuCuIoBbq5","papermill":{"duration":9.949638,"end_time":"2025-05-21T16:25:02.510764","exception":false,"start_time":"2025-05-21T16:24:52.561126","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"08bedc6f-2af6-428f-a7db-c00a24038b98","cell_type":"code","source":"helper_scripts_path = '/kaggle/input/d/leonardosandri/myhackatonhelperscripts/'\n\nif os.path.exists(helper_scripts_path):\n    # Add this path to the beginning of Python's search list\n    sys.path.insert(0, helper_scripts_path)\n    print(f\"Successfully added '{helper_scripts_path}' to sys.path.\")\n    print(f\"Contents of '{helper_scripts_path}': {os.listdir(helper_scripts_path)}\") # Verify\nelse:\n    print(f\"WARNING: Helper scripts path not found: {helper_scripts_path}\")\n    print(\"Please ensure 'myhackathonhelperscripts' dataset is correctly added to the notebook.\")\n\n# Start import of utils modules\ntry:\n    from preprocessor import MultiDatasetLoader\n    from utils import set_seed\n    # from conv import GINConv as OriginalRepoGINConv\n    from models import GNN\n    print(\"Successfully imported modules.\")\nexcept ImportError as e:\n    print(f\"ERROR importing module: {e}\")\n    print(\"Please check that the .py files exist directly under the helper_scripts_path and have no syntax errors.\")\n    # print(\"Current sys.path:\", sys.path)\n\n# Set the random seed\nset_seed()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f544afa5","cell_type":"markdown","source":"## 2. Data Preprocessing Functions\n","metadata":{}},{"id":"0a9c70d7","cell_type":"code","source":"def add_zeros(data):\n    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n    return data","metadata":{"id":"Dyf0I2-t9IcW","papermill":{"duration":0.019268,"end_time":"2025-05-21T16:25:02.544583","exception":false,"start_time":"2025-05-21T16:25:02.525315","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"f0944127","cell_type":"markdown","source":"## 3. Training and Evaluation Functions","metadata":{}},{"id":"3622cfa1","cell_type":"code","source":"def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n        data = data.to(device)\n        optimizer.zero_grad()\n        try:\n            output = model(data)\n        except IndexError as e:\n            print(f\"Error in batch with {data.num_nodes} nodes, edge_max={data.edge_index.max()}\")\n            print(f\"Batch info: x.shape={data.x.shape}, edge_index.shape={data.edge_index.shape}\")\n            raise e\n        loss = criterion(output, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n        total += data.y.size(0)\n\n    # Save checkpoints if required\n    if save_checkpoints:\n        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n        torch.save(model.state_dict(), checkpoint_file)\n        print(f\"Checkpoint saved at {checkpoint_file}\")\n\n    return total_loss / len(data_loader),  correct / total","metadata":{"id":"3jKvoQYI9Zbc","papermill":{"duration":0.019599,"end_time":"2025-05-21T16:25:02.577661","exception":false,"start_time":"2025-05-21T16:25:02.558062","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"6139b912","cell_type":"code","source":"def evaluate(data_loader, model, device, calculate_accuracy=False):\n    model.eval()\n    correct = 0\n    total = 0\n    predictions = []\n    total_loss = 0\n    criterion = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n            data = data.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            if calculate_accuracy:\n                correct += (pred == data.y).sum().item()\n                total += data.y.size(0)\n                total_loss += criterion(output, data.y).item()\n            else:\n                predictions.extend(pred.cpu().numpy())\n    if calculate_accuracy:\n        accuracy = correct / total\n        return  total_loss / len(data_loader),accuracy\n    return predictions","metadata":{"id":"8peFiIS19ZpK","papermill":{"duration":0.017908,"end_time":"2025-05-21T16:25:02.607848","exception":false,"start_time":"2025-05-21T16:25:02.589940","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"26bba939","cell_type":"markdown","source":"## 4. Utility Functions","metadata":{}},{"id":"fbdbd871","cell_type":"code","source":"def save_predictions(predictions, test_path):\n    script_dir = os.getcwd() \n    submission_folder = os.path.join(script_dir, \"submission\")\n    test_dir_name = os.path.basename(os.path.dirname(test_path))\n    \n    os.makedirs(submission_folder, exist_ok=True)\n    \n    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n    \n    test_graph_ids = list(range(len(predictions)))\n    output_df = pd.DataFrame({\n        \"id\": test_graph_ids,\n        \"pred\": predictions\n    })\n    \n    output_df.to_csv(output_csv_path, index=False)\n    print(f\"Predictions saved to {output_csv_path}\")","metadata":{"id":"WanuZKxy9Zs-","papermill":{"duration":0.016728,"end_time":"2025-05-21T16:25:02.635694","exception":false,"start_time":"2025-05-21T16:25:02.618966","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"fc3d24da","cell_type":"code","source":"def plot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n    \"\"\"\n    Plot training and validation progress over epochs.\n    \n    Args:\n        train_losses: List of training losses per epoch\n        train_accuracies: List of training accuracies per epoch  \n        val_losses: List of validation losses per epoch\n        val_accuracies: List of validation accuracies per epoch\n        output_dir: Directory to save the plot\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(15, 6))\n    \n    # Plot losses\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color='red', marker='s')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss per Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot accuracies\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green', marker='o')\n    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color='orange', marker='s')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy per Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    # Save plot\n    os.makedirs(output_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n    plt.show()\n    plt.close()","metadata":{"id":"uyHIJS5U9ZzB","papermill":{"duration":0.017765,"end_time":"2025-05-21T16:25:02.664538","exception":false,"start_time":"2025-05-21T16:25:02.646773","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"0864fa6c","cell_type":"markdown","source":"## 5. Configuration and Arguments","metadata":{}},{"id":"22574fa5","cell_type":"code","source":"def get_user_input(prompt, default=None, required=False, type_cast=str):\n\n    while True:\n        user_input = input(f\"{prompt} [{default}]: \")\n        \n        if user_input == \"\" and required:\n            print(\"This field is required. Please enter a value.\")\n            continue\n        \n        if user_input == \"\" and default is not None:\n            return default\n        \n        if user_input == \"\" and not required:\n            return None\n        \n        try:\n            return type_cast(user_input)\n        except ValueError:\n            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")","metadata":{"papermill":{"duration":0.016577,"end_time":"2025-05-21T16:25:02.692205","exception":false,"start_time":"2025-05-21T16:25:02.675628","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"139e88b2","cell_type":"code","source":"def get_arguments():\n    \"\"\"Set training configuration directly\"\"\"\n    args = {\n        # Dataset selection\n        'dataset': 'A',  # Choose: A, B, C, D\n        'train_mode': 1,  # 1=single dataset, 2=all datasets\n        \n        # Model config\n        'gnn': 'gin',  # gin, gin-virtual, gcn, gcn-virtual\n        'drop_ratio': 0.0,\n        'num_layer': 5,\n        'emb_dim': 300,\n        \n        # Training config\n        'batch_size': 32,\n        'epochs': 10,\n        'baseline_mode': 1,  # 1=CE, 2=Noisy CE\n        'noise_prob': 0.2,\n        \n        # System config\n        'device': 0,\n        'num_checkpoints': 3\n    }\n    return argparse.Namespace(**args)","metadata":{"papermill":{"duration":0.017703,"end_time":"2025-05-21T16:25:02.721184","exception":false,"start_time":"2025-05-21T16:25:02.703481","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"45bffa19","cell_type":"code","source":"def populate_args(args):\n    print(\"Arguments received:\")\n    for key, value in vars(args).items():\n        print(f\"{key}: {value}\")\nargs = get_arguments()\npopulate_args(args)","metadata":{"papermill":{"duration":0.118164,"end_time":"2025-05-21T16:25:02.850799","exception":true,"start_time":"2025-05-21T16:25:02.732635","status":"failed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d40170fe","cell_type":"markdown","source":"## 6. Loss Function Definition","metadata":{}},{"id":"09b57d62","cell_type":"code","source":"class NoisyCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, p_noisy):\n        super().__init__()\n        self.p = p_noisy\n        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        losses = self.ce(logits, targets)\n        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n        return (losses * weights).mean()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e319a162","cell_type":"markdown","source":"## 7. Main training pipeline","metadata":{}},{"id":"a210abf8","cell_type":"markdown","source":"### 7.1 Config section\n","metadata":{}},{"id":"24a82d11","cell_type":"code","source":"print(\"=\" * 60)\nprint(\"Enhanced GNN Training Pipeline\")\nprint(\"=\" * 60)\n\n# Get configuration\nargs = get_arguments()\n\nprint(\"\\nConfiguration:\")\nfor key, value in vars(args).items():\n    print(f\"  {key}: {value}\")\n\n# Setup device\ndevice = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nUsing device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"26506118","cell_type":"markdown","source":"### 7.2 Data Loading","metadata":{}},{"id":"81432ff3","cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"LOADING DATA\")\nprint(\"=\"*40)\n\nbase_path = '/kaggle/input/deep-dataset-preprocessed/processed_data_separate'\n\n# Prepare training/validation data based on mode\nif args.train_mode == 1:\n    # Single dataset mode\n    dataset_name = args.dataset\n    train_dataset = torch.load(f'{base_path}/{dataset_name}_train_graphs.pt', weights_only=False)\n    train_dataset = [add_zeros(data) for data in train_dataset]\n    \n    val_dataset = torch.load(f'{base_path}/{dataset_name}_val_graphs.pt', weights_only=False)\n    val_dataset = [add_zeros(data) for data in val_dataset]\n    \n    test_dataset = torch.load(f'{base_path}/{dataset_name}_test_graphs.pt', weights_only=False)\n    test_dataset = [add_zeros(data) for data in test_dataset]\n    print(f\"Using single dataset: {dataset_name}\")\nelse:\n    # All datasets mode\n    train_dataset = []\n    val_dataset = []\n    test_dataset = torch.load(f'{base_path}/{args.dataset}_test_graphs.pt', weights_only=False)  # Test on specified dataset\n    \n    for ds_name in ['A', 'B', 'C', 'D']:\n        train_dataset.extend(torch.load(f'{base_path}/{ds_name}_train_graphs.pt', weights_only=False))\n        val_dataset.extend(torch.load(f'{base_path}/{ds_name}_val_graphs.pt', weights_only=False))\n    \n    print(\"Using all datasets for training\")\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"63613e69","cell_type":"markdown","source":"### 7.3 Model Setup","metadata":{}},{"id":"269b5bb3","cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*40)\nprint(\"MODEL SETUP\")\nprint(\"=\"*40)\n\n# Initialize model\nif args.gnn == 'gin':\n    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, \n               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False)\nelif args.gnn == 'gin-virtual':\n    model = GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer,\n               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True)\nelif args.gnn == 'gcn':\n    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer,\n               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False)\nelif args.gnn == 'gcn-virtual':\n    model = GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer,\n               emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True)\nelse:\n    raise ValueError(f'Invalid GNN type: {args.gnn}')\n\nmodel = model.to(device)\n\n# Setup optimizer and loss\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nif args.baseline_mode == 2:\n    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n    print(f\"Using Noisy Cross Entropy Loss (p={args.noise_prob})\")\nelse:\n    criterion = torch.nn.CrossEntropyLoss()\n    print(\"Using standard Cross Entropy Loss\")\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Setup logging and checkpoints\nexp_name = f\"{args.gnn}_dataset{args.dataset}_mode{args.train_mode}\"\nlogs_dir = os.path.join(\"logs\", exp_name)\ncheckpoints_dir = os.path.join(\"checkpoints\", exp_name)\nos.makedirs(logs_dir, exist_ok=True)\nos.makedirs(checkpoints_dir, exist_ok=True)\n\n# Setup logging\nlog_file = os.path.join(logs_dir, \"training.log\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\n\nbest_model_path = os.path.join(checkpoints_dir, \"best_model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"25032fa3","cell_type":"markdown","source":"### 7.4 Training loop\n","metadata":{}},{"id":"ad0e3701","cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"TRAINING\")\nprint(\"=\"*40)\n\nbest_val_accuracy = 0.0\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\n# Calculate checkpoint intervals\nif args.num_checkpoints > 1:\n    checkpoint_intervals = [int((i + 1) * args.epochs / args.num_checkpoints) \n                          for i in range(args.num_checkpoints)]\nelse:\n    checkpoint_intervals = [args.epochs]\n\nfor epoch in range(args.epochs):\n    print(f\"\\nEpoch {epoch + 1}/{args.epochs}\")\n    print(\"-\" * 30)\n    \n    # Training\n    train_loss, train_acc = train(\n        train_loader, model, optimizer, criterion, device,\n        save_checkpoints=(epoch + 1 in checkpoint_intervals),\n        checkpoint_path=os.path.join(checkpoints_dir, \"checkpoint\"),\n        current_epoch=epoch\n    )\n    \n    # Validation\n    val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True)\n    \n    # Log results\n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    logging.info(f\"Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n                f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n    \n    # Store metrics\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    # Save best model\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"★ New best model saved! Val Acc: {val_acc:.4f}\")\n\nprint(f\"\\nBest validation accuracy: {best_val_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f994101a-2113-4a64-bbd6-17acd9f5988a","cell_type":"code","source":"# Plot training progress\nplot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, logs_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"25604a74","cell_type":"markdown","source":"### 7.5 Testing and predictions","metadata":{}},{"id":"de27e926","cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"TESTING\")\nprint(\"=\"*40)\n\n# Load best model and make predictions\nmodel.load_state_dict(torch.load(best_model_path))\nprint(f\"Loaded best model from: {best_model_path}\")\n\npredictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n\n# Save predictions\nsave_predictions(predictions, args.dataset)\n\n# Cleanup for memory\ndel train_dataset, val_dataset, test_dataset\ndel train_loader, val_loader, test_loader\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*60)\nprint(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\nprint(f\"Predictions saved for dataset {args.dataset}\")\nprint(f\"Logs and plots saved in: {logs_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}