{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:18:00.109141Z",
     "iopub.status.busy": "2025-05-24T20:18:00.108806Z",
     "iopub.status.idle": "2025-05-24T20:18:14.123665Z",
     "shell.execute_reply": "2025-05-24T20:18:14.122815Z",
     "shell.execute_reply.started": "2025-05-24T20:18:00.109122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported GraphDataset and set_seed.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv as PyG_GINConv, global_add_pool\n",
    "\n",
    "#Set up path to the src directory for import of pre-made teacher modules\n",
    "src_path = os.path.join(os.path.abspath('.'), 'hackaton', 'src')\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "try:\n",
    "    from loadData import GraphDataset\n",
    "    from utils import set_seed\n",
    "    # from conv import GINConv as OriginalRepoGINConv\n",
    "    # from models import GNN as OriginalRepoGNN\n",
    "    print(\"Successfully imported GraphDataset and set_seed.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR importing module: {e}\")\n",
    "    print(\"Please check that the .py files exist and have no syntax errors.\")\n",
    "    # print(\"Current sys.path:\", sys.path)\n",
    "\n",
    "\n",
    "# Call set_seed early\n",
    "set_seed(42) # You can change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:18:14.124989Z",
     "iopub.status.busy": "2025-05-24T20:18:14.124520Z",
     "iopub.status.idle": "2025-05-24T20:18:14.128593Z",
     "shell.execute_reply": "2025-05-24T20:18:14.127960Z",
     "shell.execute_reply.started": "2025-05-24T20:18:14.124954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_zeros(data):\n",
    "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:30.363663Z",
     "iopub.status.busy": "2025-05-24T20:19:30.363052Z",
     "iopub.status.idle": "2025-05-24T20:19:30.376792Z",
     "shell.execute_reply": "2025-05-24T20:19:30.375936Z",
     "shell.execute_reply.started": "2025-05-24T20:19:30.363628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImprovedGIN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_gnn_layers=3, dropout_rate=0.5):\n",
    "        super(ImprovedGIN, self).__init__()\n",
    "        # Embedding for nodes if data.x comes from add_zeros (all zeros)\n",
    "        # This means we learn a single feature vector for all nodes initially.\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=1, embedding_dim=input_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Initial GIN layer's MLP: maps from input_dim (from embedding) to hidden_dim\n",
    "        # PyG_GINConv takes an nn.Sequential (MLP) as its first argument.\n",
    "        # The MLP's input dim should be input_dim, output dim can be hidden_dim.\n",
    "        mlp_initial = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim) # GIN paper suggests MLP output dim == GINConv output dim\n",
    "        )\n",
    "        self.convs.append(PyG_GINConv(mlp_initial, train_eps=True)) # train_eps=True is common for GIN\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Subsequent GIN layers: map from hidden_dim to hidden_dim\n",
    "        for _ in range(1, self.num_gnn_layers):\n",
    "            mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.convs.append(PyG_GINConv(mlp, train_eps=True))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.global_pool = global_add_pool # Using global_add_pool\n",
    "\n",
    "        # Classifier: A 2-layer MLP\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = self.embedding(x) # Shape: (num_nodes, input_dim)\n",
    "        \n",
    "        residual_x = None # To store the output for residual connection\n",
    "\n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x_conv_input = x\n",
    "            # For GIN with train_eps=True, the original node features are added internally by the layer\n",
    "            # based on (1+eps)*x_orig + aggregated_neighbors.\n",
    "            # So the residual connection should ideally be applied after the GINConv operation if mimicking GIN paper's layer.\n",
    "            # Or, a simpler skip connection: input to layer i is output of layer i-1.\n",
    "            \n",
    "            x = self.convs[i](x_conv_input, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            \n",
    "            # Add residual connection\n",
    "            if residual_x is not None and i > 0 : # Add output of previous layer (after activation and dropout)\n",
    "                 x = x + residual_x\n",
    "            \n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "            residual_x = x # Store for next layer's residual connection\n",
    "\n",
    "        x_pooled = self.global_pool(x, batch)\n",
    "\n",
    "        x_pooled = F.dropout(x_pooled, p=self.dropout_rate, training=self.training)\n",
    "        x_fc1 = self.fc1(x_pooled)\n",
    "        x_relu_fc1 = F.relu(x_fc1)\n",
    "        x_dropout_fc1 = F.dropout(x_relu_fc1, p=self.dropout_rate, training=self.training)\n",
    "        out = self.fc2(x_dropout_fc1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"ImprovedGIN model class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:30.377796Z",
     "iopub.status.busy": "2025-05-24T20:19:30.377527Z",
     "iopub.status.idle": "2025-05-24T20:19:32.341768Z",
     "shell.execute_reply": "2025-05-24T20:19:32.340935Z",
     "shell.execute_reply.started": "2025-05-24T20:19:30.377773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImprovedGIN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_gnn_layers=3, dropout_rate=0.5):\n",
    "        super(ImprovedGIN, self).__init__()\n",
    "        # Embedding for nodes if data.x comes from add_zeros (all zeros)\n",
    "        # This means we learn a single feature vector for all nodes initially.\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=1, embedding_dim=input_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Initial GIN layer's MLP: maps from input_dim (from embedding) to hidden_dim\n",
    "        # PyG_GINConv takes an nn.Sequential (MLP) as its first argument.\n",
    "        # The MLP's input dim should be input_dim, output dim can be hidden_dim.\n",
    "        mlp_initial = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim) # GIN paper suggests MLP output dim == GINConv output dim\n",
    "        )\n",
    "        self.convs.append(PyG_GINConv(mlp_initial, train_eps=True)) # train_eps=True is common for GIN\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Subsequent GIN layers: map from hidden_dim to hidden_dim\n",
    "        for _ in range(1, self.num_gnn_layers):\n",
    "            mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.convs.append(PyG_GINConv(mlp, train_eps=True))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.global_pool = global_add_pool # Using global_add_pool\n",
    "\n",
    "        # Classifier: A 2-layer MLP\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = self.embedding(x) # Shape: (num_nodes, input_dim)\n",
    "        \n",
    "        residual_x = None # To store the output for residual connection\n",
    "\n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x_conv_input = x\n",
    "            # For GIN with train_eps=True, the original node features are added internally by the layer\n",
    "            # based on (1+eps)*x_orig + aggregated_neighbors.\n",
    "            # So the residual connection should ideally be applied after the GINConv operation if mimicking GIN paper's layer.\n",
    "            # Or, a simpler skip connection: input to layer i is output of layer i-1.\n",
    "            \n",
    "            x = self.convs[i](x_conv_input, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            \n",
    "            # Add residual connection\n",
    "            if residual_x is not None and i > 0 : # Add output of previous layer (after activation and dropout)\n",
    "                 x = x + residual_x\n",
    "            \n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "            residual_x = x # Store for next layer's residual connection\n",
    "\n",
    "        x_pooled = self.global_pool(x, batch)\n",
    "\n",
    "        x_pooled = F.dropout(x_pooled, p=self.dropout_rate, training=self.training)\n",
    "        x_fc1 = self.fc1(x_pooled)\n",
    "        x_relu_fc1 = F.relu(x_fc1)\n",
    "        x_dropout_fc1 = F.dropout(x_relu_fc1, p=self.dropout_rate, training=self.training)\n",
    "        out = self.fc2(x_dropout_fc1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"ImprovedGIN model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:32.342956Z",
     "iopub.status.busy": "2025-05-24T20:19:32.342659Z",
     "iopub.status.idle": "2025-05-24T20:19:32.360227Z",
     "shell.execute_reply": "2025-05-24T20:19:32.359536Z",
     "shell.execute_reply.started": "2025-05-24T20:19:32.342930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, \n",
    "          save_checkpoints_periodically, periodic_checkpoint_dir_base, \n",
    "          dataset_name_for_checkpoint, current_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for data in tqdm(data_loader, desc=f\"Epoch {current_epoch+1} Training\", unit=\"batch\", leave=False):\n",
    "        data = data.to(device)\n",
    "        if data.y is None: # Skip if no labels (should not happen in training)\n",
    "            continue\n",
    "        if data.y.numel() == 0: # Skip if batch is empty or labels are empty\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * data.num_graphs # Loss is mean over batch, scale by batch size\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct_preds += (pred == data.y).sum().item()\n",
    "        total_samples += data.y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "    accuracy = correct_preds / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    if save_checkpoints_periodically:\n",
    "        # e.g. checkpoints/A/model_A_epoch_10.pth\n",
    "        final_periodic_checkpoint_dir = os.path.join(periodic_checkpoint_dir_base, dataset_name_for_checkpoint)\n",
    "        os.makedirs(final_periodic_checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_file = os.path.join(final_periodic_checkpoint_dir, f\"model_{dataset_name_for_checkpoint}_epoch_{current_epoch + 1}.pth\")\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        logging.info(f\"Periodic checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"train function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:32.361071Z",
     "iopub.status.busy": "2025-05-24T20:19:32.360911Z",
     "iopub.status.idle": "2025-05-24T20:19:32.372875Z",
     "shell.execute_reply": "2025-05-24T20:19:32.372338Z",
     "shell.execute_reply.started": "2025-05-24T20:19:32.361058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, criterion_for_loss=None, calculate_accuracy_and_loss=False):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    predictions_list = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\", leave=False):\n",
    "            data = data.to(device)\n",
    "            if data.y is None and calculate_accuracy_and_loss: # Cannot calculate accuracy without labels\n",
    "                continue\n",
    "            if data.y is not None and data.y.numel() == 0 and calculate_accuracy_and_loss:\n",
    "                 continue\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy_and_loss and data.y is not None:\n",
    "                correct_preds += (pred == data.y).sum().item()\n",
    "                total_samples += data.y.size(0)\n",
    "                if criterion_for_loss:\n",
    "                    # Ensure criterion handles cases where output might be for fewer samples than data.y if some were filtered\n",
    "                    valid_indices = (data.y != -1) # Example: if -1 indicates no label for a sample\n",
    "                    if output.size(0) == data.y[valid_indices].size(0): # Check if output matches filtered labels\n",
    "                         total_loss += criterion_for_loss(output, data.y[valid_indices]).item() * output.size(0)\n",
    "                    elif output.size(0) == data.y.size(0):\n",
    "                         total_loss += criterion_for_loss(output, data.y).item() * data.num_graphs\n",
    "\n",
    "\n",
    "            else: # Only collecting predictions for test set usually\n",
    "                predictions_list.extend(pred.cpu().numpy())\n",
    "\n",
    "    if calculate_accuracy_and_loss:\n",
    "        accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 and criterion_for_loss else 0.0\n",
    "        return avg_loss, accuracy\n",
    "    return predictions_list\n",
    "\n",
    "print(\"evaluate function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:32.374036Z",
     "iopub.status.busy": "2025-05-24T20:19:32.373793Z",
     "iopub.status.idle": "2025-05-24T20:19:32.392856Z",
     "shell.execute_reply": "2025-05-24T20:19:32.392295Z",
     "shell.execute_reply.started": "2025-05-24T20:19:32.374014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(predictions_to_save, test_path_arg, root_submission_folder=\"submission\", id_col_name=\"id\", pred_col_name=\"pred\"):\n",
    "    current_dir = os.getcwd() # Use current_dir for clarity\n",
    "    submission_folder_main_path = os.path.join(current_dir, root_submission_folder)\n",
    "    os.makedirs(submission_folder_main_path, exist_ok=True)\n",
    "\n",
    "    dataset_folder_name = \"datasets\"\n",
    "    if test_path_arg:\n",
    "        try:\n",
    "            # Assumes test_path_arg is like './datasets/A/test.json.gz'\n",
    "            dataset_folder_name = os.path.basename(os.path.dirname(test_path_arg))\n",
    "            if not dataset_folder_name or dataset_folder_name == \".\": # Handle edge cases\n",
    "                # if path is just 'test.json.gz' or './test.json.gz'\n",
    "                path_parts = os.path.normpath(test_path_arg).split(os.sep)\n",
    "                if len(path_parts) > 2 and path_parts[-2] != \"datasets\": # e.g. datasets/A/file\n",
    "                    dataset_folder_name = path_parts[-2]\n",
    "                elif len(path_parts) > 1 and path_parts[-2] == \"datasets\": # e.g. datasets/file (no subfolder)\n",
    "                     dataset_folder_name = \"root_dataset\"\n",
    "                else: # single file\n",
    "                    dataset_folder_name = \"direct_file_dataset\"\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing dataset name from test_path: {e}, using default.\")\n",
    "            \n",
    "    output_csv_filename = f\"testset_{dataset_folder_name}.csv\"\n",
    "    output_csv_full_path = os.path.join(submission_folder_main_path, output_csv_filename)\n",
    "    \n",
    "    # Assuming predictions are for sequentially indexed graphs\n",
    "    graph_ids = list(range(len(predictions_to_save)))\n",
    "    output_df = pd.DataFrame({\n",
    "        id_col_name: graph_ids,\n",
    "        pred_col_name: predictions_to_save\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_full_path, index=False)\n",
    "    logging.info(f\"Predictions saved to {output_csv_full_path}\")\n",
    "    print(f\"Predictions saved to {output_csv_full_path}\")\n",
    "\n",
    "print(\"save_predictions function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:32.395280Z",
     "iopub.status.busy": "2025-05-24T20:19:32.394979Z",
     "iopub.status.idle": "2025-05-24T20:19:32.409444Z",
     "shell.execute_reply": "2025-05-24T20:19:32.408824Z",
     "shell.execute_reply.started": "2025-05-24T20:19:32.395263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(losses_data, accuracies_data, plot_title_prefix, output_dir_for_plots, dataset_name_for_plot):\n",
    "    epochs_count = range(1, len(losses_data) + 1)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5)) # Use subplots for clarity\n",
    "\n",
    "    # Plot Loss\n",
    "    axs[0].plot(epochs_count, losses_data, 'b-o', label=f\"{plot_title_prefix} Loss\")\n",
    "    axs[0].set_title(f'{plot_title_prefix} Loss ({dataset_name_for_plot})')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    axs[1].plot(epochs_count, accuracies_data, 'g-o', label=f\"{plot_title_prefix} Accuracy\")\n",
    "    axs[1].set_title(f'{plot_title_prefix} Accuracy ({dataset_name_for_plot})')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    os.makedirs(output_dir_for_plots, exist_ok=True)\n",
    "    plot_filename = f\"{plot_title_prefix.lower().replace(' ', '_')}_{dataset_name_for_plot}_progress.png\"\n",
    "    fig.savefig(os.path.join(output_dir_for_plots, plot_filename))\n",
    "    plt.show() # Show plot in notebook\n",
    "    plt.close(fig) # Close the figure to free memory\n",
    "    logging.info(f\"Plot saved: {os.path.join(output_dir_for_plots, plot_filename)}\")\n",
    "\n",
    "print(\"plot_training_progress function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:32.410466Z",
     "iopub.status.busy": "2025-05-24T20:19:32.410231Z",
     "iopub.status.idle": "2025-05-24T20:19:32.428032Z",
     "shell.execute_reply": "2025-05-24T20:19:32.427491Z",
     "shell.execute_reply.started": "2025-05-24T20:19:32.410451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ExperimentConfig:\n",
    "    def __init__(self):\n",
    "        # --- Crucial: Dataset Paths ---\n",
    "        # Ensure these paths are correct for your setup.\n",
    "        # Example: To run for dataset 'A'\n",
    "        self.dataset_letter = 'A' # CHANGE THIS FOR DIFFERENT DATASETS (A, B, C, D)\n",
    "        \n",
    "        # Base path to where 'A', 'B', etc. folders are located\n",
    "        self.base_dataset_path = \"./data\"\n",
    "        self.train_path = os.path.join(self.base_dataset_path, self.dataset_letter, \"train.json.gz\")\n",
    "        self.test_path = os.path.join(self.base_dataset_path, self.dataset_letter, \"test.json.gz\")\n",
    "        # Set self.train_path = None to skip training and only test (requires a best_model checkpoint)\n",
    "        # self.train_path = None \n",
    "\n",
    "        # --- Model Hyperparameters (for ImprovedGIN) ---\n",
    "        self.input_dim = 300         # For the embedding layer with add_zeros\n",
    "        self.hidden_dim = 128        # Hidden dimension for GIN layers\n",
    "        self.num_gnn_layers = 3      # Number of GIN layers in ImprovedGIN\n",
    "        self.dropout_rate = 0.3      # Dropout rate used in ImprovedGIN\n",
    "\n",
    "        # --- Training Hyperparameters ---\n",
    "        self.epochs = 20             # Number of epochs to train (increased from 10 for better model)\n",
    "        self.batch_size = 32\n",
    "        self.lr = 0.001              # Learning rate\n",
    "\n",
    "        # --- Loss Function ---\n",
    "        # Options: \"CE\" for CrossEntropyLoss, \"NoisyCE\" for NoisyCrossEntropyLoss\n",
    "        self.loss_type = \"CE\"\n",
    "        self.num_classes = 6         # Required for NoisyCrossEntropyLoss if used, and model output\n",
    "        self.noise_prob = 0.1        # p_noisy for NoisyCrossEntropyLoss (if self.loss_type is \"NoisyCE\")\n",
    "\n",
    "        # --- Checkpoints & Logging & Submission ---\n",
    "        self.num_periodic_checkpoints = 3 # How many checkpoints to save during training epochs\n",
    "                                         # (e.g., 3 means save at 1/3, 2/3, and end of epochs)\n",
    "                                         # The 'best' model based on validation is always saved.\n",
    "        self.base_checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "        self.base_log_dir = \"/kaggle/working/logs\"\n",
    "        self.base_submission_dir = \"/kaggle/working/submission\"\n",
    "\n",
    "        # --- Device ---\n",
    "        self.force_cpu = False # Set to True to force CPU even if CUDA is available\n",
    "\n",
    "# Instantiate the configuration\n",
    "config = ExperimentConfig()\n",
    "\n",
    "# --- Validate paths ---\n",
    "if config.train_path and not os.path.exists(config.train_path):\n",
    "    print(f\"WARNING: Train path does not exist: {config.train_path}\")\n",
    "    # config.train_path = None # Optionally disable training if path missing\n",
    "if not os.path.exists(config.test_path):\n",
    "    raise FileNotFoundError(f\"CRITICAL: Test path does not exist: {config.test_path}\")\n",
    "\n",
    "print(f\"Configuration for Dataset '{config.dataset_letter}':\")\n",
    "for key, value in vars(config).items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T20:19:32.428871Z",
     "iopub.status.busy": "2025-05-24T20:19:32.428683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- 0. Setup Device ----\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and not config.force_cpu else 'cpu')\n",
    "print(f\"========= Running on Device: {device} =========\")\n",
    "\n",
    "# ---- 1. Determine Dataset Name for file/folder naming ----\n",
    "# This is now handled by config.dataset_letter directly.\n",
    "dataset_name_fs = config.dataset_letter # Filesystem-safe dataset name\n",
    "print(f\"========= Processing Dataset: {dataset_name_fs} =========\")\n",
    "\n",
    "# ---- 2. Setup Output Directories ----\n",
    "# Checkpoints: base_checkpoint_dir / dataset_name_fs / model_files... (periodic)\n",
    "#            also base_checkpoint_dir / model_DATASET_best.pth (best val)\n",
    "# Logs: base_log_dir / dataset_name_fs / logfile.log & plots...\n",
    "# Submission: base_submission_dir / testset_DATASET.csv\n",
    "\n",
    "dataset_periodic_checkpoint_storage_dir = os.path.join(config.base_checkpoint_dir, dataset_name_fs)\n",
    "os.makedirs(dataset_periodic_checkpoint_storage_dir, exist_ok=True)\n",
    "best_model_overall_checkpoint_path = os.path.join(config.base_checkpoint_dir, f\"model_{dataset_name_fs}_best.pth\")\n",
    "\n",
    "dataset_specific_log_dir = os.path.join(config.base_log_dir, dataset_name_fs)\n",
    "os.makedirs(dataset_specific_log_dir, exist_ok=True)\n",
    "main_log_file_path = os.path.join(dataset_specific_log_dir, \"experiment.log\")\n",
    "\n",
    "# ---- 3. Setup Logging ----\n",
    "# Remove existing handlers to prevent duplicate logs if re-running cell\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(main_log_file_path, mode='w'), # Overwrite log file each run for this dataset\n",
    "        logging.StreamHandler() # Also print to console\n",
    "    ]\n",
    ")\n",
    "logging.info(f\"Logging setup for dataset {dataset_name_fs}. Log file: {main_log_file_path}\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "for key, value in vars(config).items(): # Log the config\n",
    "    logging.info(f\"CONFIG - {key}: {value}\")\n",
    "\n",
    "\n",
    "# ---- 4. Initialize Model, Optimizer, Criterion ----\n",
    "model = ImprovedGIN(\n",
    "    input_dim=config.input_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    "    output_dim=config.num_classes,\n",
    "    num_gnn_layers=config.num_gnn_layers,\n",
    "    dropout_rate=config.dropout_rate\n",
    ").to(device)\n",
    "logging.info(f\"Model: ImprovedGIN initialized: layers={config.num_gnn_layers}, hidden_dim={config.hidden_dim}, dropout={config.dropout_rate}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "logging.info(f\"Optimizer: Adam with LR={config.lr}\")\n",
    "\n",
    "if config.loss_type.upper() == \"NOISYCE\":\n",
    "    criterion = NoisyCrossEntropyLoss(p_noisy=config.noise_prob, num_classes=config.num_classes)\n",
    "    logging.info(f\"Using NoisyCrossEntropyLoss with p_noisy={config.noise_prob}\")\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    logging.info(\"Using standard CrossEntropyLoss\")\n",
    "\n",
    "# ---- 5. Data Loading ----\n",
    "if not os.path.exists(config.test_path):\n",
    "    logging.error(f\"Test path {config.test_path} not found. Exiting.\")\n",
    "    raise FileNotFoundError(f\"Test path {config.test_path} not found.\")\n",
    "# TODO: modify the path of cnofig.test_path or remove modifications to use_processed and loadData\n",
    "test_dataset = GraphDataset(config.test_path, transform=add_zeros, use_processed=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "logging.info(f\"Loaded test data from {config.test_path} ({len(test_dataset)} graphs)\")\n",
    "\n",
    "# ---- 6. Training Phase ----\n",
    "if config.train_path and os.path.exists(config.train_path):\n",
    "    logging.info(f\"Loading training data from {config.train_path}\")\n",
    "    full_train_dataset_obj = GraphDataset(config.train_path, transform=add_zeros)\n",
    "    logging.info(f\"Full training dataset loaded ({len(full_train_dataset_obj)} graphs)\")\n",
    "    \n",
    "    val_split_ratio = 0.2 # 20% for validation\n",
    "    num_total_train_graphs = len(full_train_dataset_obj)\n",
    "    num_val_graphs = int(val_split_ratio * num_total_train_graphs)\n",
    "    num_train_graphs = num_total_train_graphs - num_val_graphs\n",
    "\n",
    "    split_generator = torch.Generator().manual_seed(42) # For reproducible splits\n",
    "    train_dataset_split_obj, val_dataset_split_obj = torch.utils.data.random_split(\n",
    "        full_train_dataset_obj, [num_train_graphs, num_val_graphs], generator=split_generator\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset_split_obj, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset_split_obj, batch_size=config.batch_size, shuffle=False)\n",
    "    logging.info(f\"Training data split: {len(train_dataset_split_obj)} train, {len(val_dataset_split_obj)} val graphs.\")\n",
    "\n",
    "    best_val_accuracy = -1.0 # Initialize to a low value\n",
    "    train_losses_hist, train_accuracies_hist = [], []\n",
    "    val_losses_hist, val_accuracies_hist = [], []\n",
    "\n",
    "    # Determine epochs for periodic checkpoint saving\n",
    "    epochs_for_periodic_checkpoints = set()\n",
    "    if config.num_periodic_checkpoints > 0 and config.epochs > 0:\n",
    "        periodic_interval = config.epochs // config.num_periodic_checkpoints\n",
    "        if periodic_interval > 0:\n",
    "            for i in range(1, config.num_periodic_checkpoints + 1):\n",
    "                epochs_for_periodic_checkpoints.add(i * periodic_interval)\n",
    "    epochs_for_periodic_checkpoints.add(config.epochs) # Always save at the final epoch\n",
    "\n",
    "    logging.info(f\"Starting training for {config.epochs} epochs...\")\n",
    "    for epoch in range(config.epochs):\n",
    "        save_this_epoch_periodically = (epoch + 1) in epochs_for_periodic_checkpoints\n",
    "        \n",
    "        avg_epoch_train_loss, epoch_train_acc = train(\n",
    "            train_loader, model, optimizer, criterion, device,\n",
    "            save_checkpoints_periodically=save_this_epoch_periodically,\n",
    "            periodic_checkpoint_dir_base=config.base_checkpoint_dir,\n",
    "            dataset_name_for_checkpoint=dataset_name_fs,\n",
    "            current_epoch=epoch\n",
    "        )\n",
    "        \n",
    "        avg_epoch_val_loss, epoch_val_acc = evaluate(\n",
    "            val_loader, model, device, \n",
    "            criterion_for_loss=criterion, calculate_accuracy_and_loss=True\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Epoch {epoch+1}/{config.epochs} -> Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        train_losses_hist.append(avg_epoch_train_loss)\n",
    "        train_accuracies_hist.append(epoch_train_acc)\n",
    "        val_losses_hist.append(avg_epoch_val_loss)\n",
    "        val_accuracies_hist.append(epoch_val_acc)\n",
    "\n",
    "        if epoch_val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_acc\n",
    "            torch.save(model.state_dict(), best_model_overall_checkpoint_path)\n",
    "            logging.info(f\"*** New best validation accuracy: {best_val_accuracy:.4f}. Best model saved to {best_model_overall_checkpoint_path} ***\")\n",
    "    \n",
    "    logging.info(\"Training finished.\")\n",
    "    # Plotting training and validation progress\n",
    "    plots_output_directory = os.path.join(dataset_specific_log_dir, \"plots\")\n",
    "    plot_training_progress(train_losses_hist, train_accuracies_hist, \"Training\", plots_output_directory, dataset_name_fs)\n",
    "    plot_training_progress(val_losses_hist, val_accuracies_hist, \"Validation\", plots_output_directory, dataset_name_fs)\n",
    "    logging.info(f\"Training & validation plots saved to {plots_output_directory}\")\n",
    "\n",
    "    # Clean up large objects from memory\n",
    "    del full_train_dataset_obj, train_dataset_split_obj, val_dataset_split_obj, train_loader, val_loader\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "elif not config.train_path :\n",
    "    logging.info(\"config.train_path is None. Skipping training. Model will be loaded for testing.\")\n",
    "else: # train_path was specified but not found\n",
    "     logging.warning(f\"Training path {config.train_path} was specified but not found. Skipping training. Attempting to load best model for testing.\")\n",
    "\n",
    "\n",
    "# ---- 7. Testing Phase ----\n",
    "if not os.path.exists(best_model_overall_checkpoint_path):\n",
    "    logging.error(f\"Best model checkpoint {best_model_overall_checkpoint_path} not found. Cannot proceed with testing. Ensure training was run or a valid checkpoint exists.\")\n",
    "    raise FileNotFoundError(f\"Best model checkpoint {best_model_overall_checkpoint_path} not found.\")\n",
    "\n",
    "logging.info(f\"Loading best model for testing from: {best_model_overall_checkpoint_path}\")\n",
    "try:\n",
    "    # Ensure model is on the correct device before loading state_dict if it was saved from a different device\n",
    "    model.to(device) \n",
    "    model.load_state_dict(torch.load(best_model_overall_checkpoint_path, map_location=device))\n",
    "    logging.info(\"Best model loaded successfully for testing.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load best model from {best_model_overall_checkpoint_path}: {e}\")\n",
    "    raise\n",
    "\n",
    "final_test_predictions = evaluate(test_loader, model, device, calculate_accuracy_and_loss=False)\n",
    "\n",
    "save_predictions(final_test_predictions, config.test_path, root_submission_folder=config.base_submission_dir)\n",
    "logging.info(f\"========= Experiment for dataset {dataset_name_fs} finished. =========\")\n",
    "\n",
    "# Clean up console handler for potential re-runs of the cell\n",
    "logging.getLogger().removeHandler(console_handler) # Assumes console_handler was defined if used"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7503117,
     "sourceId": 11934327,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7504895,
     "sourceId": 11937135,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DeepTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
