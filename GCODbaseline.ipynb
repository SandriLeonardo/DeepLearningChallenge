{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11958675,"sourceType":"datasetVersion","datasetId":7519186},{"sourceId":11998952,"sourceType":"datasetVersion","datasetId":7519473}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import random_split\nimport argparse\nimport torch.nn.functional as F\nfrom torch_geometric.utils import dropout_edge\nimport torch.nn as nn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"helper_scripts_path = '/kaggle/input/myhackatonhelperscripts'\n\nif os.path.exists(helper_scripts_path):\n    # Add this path to the beginning of Python's search list\n    sys.path.insert(0, helper_scripts_path)\n    print(f\"Successfully added '{helper_scripts_path}' to sys.path.\")\n    print(f\"Contents of '{helper_scripts_path}': {os.listdir(helper_scripts_path)}\") # Verify\nelse:\n    print(f\"WARNING: Helper scripts path not found: {helper_scripts_path}\")\n    print(\"Please ensure 'myhackathonhelperscripts' dataset is correctly added to the notebook.\")\n\n# Start import of utils modules\ntry:\n    from preprocessor import MultiDatasetLoader\n    from utils import set_seed\n    # from conv import GINConv as OriginalRepoGINConv\n    from models_EDandBatch_norm import GNN\n    print(\"Successfully imported modules.\")\nexcept ImportError as e:\n    print(f\"ERROR importing module: {e}\")\n    print(\"Please check that the .py files exist directly under the helper_scripts_path and have no syntax errors.\")\n    # print(\"Current sys.path:\", sys.path)\n\n# Set the random seed\nset_seed()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_zeros(data):\n    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In the cell with 'def train(...)'\n\ndef train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch,\n          args_namespace, u_values_global, current_baseline_mode, scheduler=None): # Added new params\n    model.train()\n    total_loss_accum = 0.0 \n    correct_preds = 0 \n    total_samples_processed = 0 \n\n    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n        data = data.to(device)\n        optimizer.zero_grad()\n\n        output = model(data)\n\n        if current_baseline_mode == 4: # GCOD specific logic\n            # Corrected line for batch_indices:\n            batch_indices = data.original_idx.to(device=device, dtype=torch.long)\n            \n            # Ensure u_values_global is on the correct device if batch_indices are used directly\n            # If u_values_global is on CPU, then indices should be CPU too before indexing.\n            # Assuming u_values_global is already on the same device as model/data or CPU (handled by .to(device) below)\n            if u_values_global.device != device: # If u_values_global is on CPU and device is GPU\n                 u_batch_cpu = u_values_global[batch_indices.cpu()].clone().detach()\n                 u_batch = u_batch_cpu.to(device).requires_grad_(True)\n            else: # If u_values_global is already on the target device\n                 u_batch = u_values_global[batch_indices].clone().detach().requires_grad_(True)\n\n\n            # Using the 'output' from the main model pass for u_optimization as well.\n            # Detach it to prevent gradients from L2 flowing back to model parameters during u-opt.\n            output_for_u_optim = output.detach()\n\n            for _ in range(args_namespace.gcod_T_u):\n                # Corrected condition for checking gradient:\n                if u_batch.grad is not None:\n                    u_batch.grad.zero_()\n                \n                L2_for_u = criterion.compute_L2(output_for_u_optim, data.y, u_batch)\n                \n                L2_for_u.backward() \n                with torch.no_grad():\n                    u_batch.data -= args_namespace.gcod_lr_u * u_batch.grad.data\n                    u_batch.data.clamp_(0, 1) \n            \n            u_batch_optimized = u_batch.detach() \n\n            pred_for_acc = output.argmax(dim=1)\n            if data.y.size(0) > 0:\n                batch_accuracy = (pred_for_acc == data.y).sum().item() / data.y.size(0)\n            else:\n                batch_accuracy = 0.0\n\n            loss_theta_components = criterion(output, data.y, u_batch_optimized, batch_accuracy)\n            actual_loss_for_bp = loss_theta_components[0] \n\n            with torch.no_grad():\n                # Ensure batch_indices are on the same device as u_values_global for assignment\n                if u_values_global.device != device:\n                    u_values_global[batch_indices.cpu()] = u_batch_optimized.cpu()\n                else:\n                    u_values_global[batch_indices] = u_batch_optimized\n        \n        else: \n            actual_loss_for_bp = criterion(output, data.y)\n        \n        try:\n            actual_loss_for_bp.backward()\n            optimizer.step()\n            # Step OneCycleLR scheduler after each batch\n            if scheduler is not None and args.scheduler_type == 'OneCycleLR':\n                scheduler.step()\n        except IndexError as e:\n            edge_max_val = data.edge_index.max().item() if data.edge_index.numel() > 0 else 'N/A'\n            print(f\"Error in batch with {data.num_nodes} nodes, edge_max={edge_max_val}\")\n            print(f\"Batch info: x.shape={data.x.shape}, edge_index.shape={data.edge_index.shape}\")\n            if current_baseline_mode == 4:\n                print(f\"GCOD context: u_batch_optimized shape: {u_batch_optimized.shape if 'u_batch_optimized' in locals() else 'N/A'}\")\n            raise e\n        \n        total_loss_accum += actual_loss_for_bp.item() \n        \n        pred_final = output.argmax(dim=1)\n        correct_preds += (pred_final == data.y).sum().item()\n        total_samples_processed += data.y.size(0)\n\n    if save_checkpoints: \n        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n        torch.save(model.state_dict(), checkpoint_file)\n        print(f\"Checkpoint saved at {checkpoint_file}\")\n\n    avg_loss = total_loss_accum / len(data_loader) if len(data_loader) > 0 else 0.0\n    accuracy = correct_preds / total_samples_processed if total_samples_processed > 0 else 0.0\n    return avg_loss, accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 7 (Corrected)\n# def evaluate(data_loader, model, criterion, device, calculate_accuracy=False): # Original\ndef evaluate(data_loader, model, criterion, device, calculate_accuracy=False, args_namespace=None, u_values_global_eval=None): # Added args_namespace and u_values for eval if needed\n    model.eval()\n    correct = 0\n    total = 0\n    predictions_list = [] # Renamed\n    total_loss_val = 0 # Renamed\n\n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n            data = data.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n\n            if calculate_accuracy:\n                correct += (pred == data.y).sum().item()\n                total += data.y.size(0)\n                \n                # Loss calculation\n                if args_namespace and args_namespace.baseline_mode == 4:\n                    # For GCOD evaluation, use L1 with u=0 as a proxy.\n                    # GCODLoss.compute_L1(self, logits, targets, u_params)\n                    u_eval_dummy = torch.zeros(data.y.size(0), device=device, dtype=torch.float)\n                    # If you trained u_values for validation set and want to use them:\n                    # batch_indices_eval = torch.tensor(data.original_idx, dtype=torch.long).to(device)\n                    # u_eval_dummy = u_values_global_eval[batch_indices_eval].clone().detach().to(device)\n\n                    loss_value = criterion.compute_L1(output, data.y, u_eval_dummy)\n                else:\n                    loss_value = criterion(output, data.y) # Standard call for other losses\n                total_loss_val += loss_value.item()\n            else:\n                predictions_list.extend(pred.cpu().numpy()) # Renamed\n\n    if calculate_accuracy:\n        accuracy = correct / total if total > 0 else 0.0\n        avg_loss = total_loss_val / len(data_loader) if len(data_loader) > 0 else 0.0\n        return avg_loss, accuracy\n    return predictions_list # Renamed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_progress(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n    \"\"\"\n    Plot training and validation progress over epochs.\n    \n    Args:\n        train_losses: List of training losses per epoch\n        train_accuracies: List of training accuracies per epoch  \n        val_losses: List of validation losses per epoch\n        val_accuracies: List of validation accuracies per epoch\n        output_dir: Directory to save the plot\n    \"\"\"\n    epochs = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(15, 6))\n    \n    # Plot losses\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue', marker='o')\n    plt.plot(epochs, val_losses, label=\"Validation Loss\", color='red', marker='s')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss per Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot accuracies\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green', marker='o')\n    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color='orange', marker='s')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy per Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    # Save plot\n    os.makedirs(output_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n    plt.show()\n    plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_predictions(predictions, test_path):\n    script_dir = os.getcwd() \n    submission_folder = os.path.join(script_dir, \"submission\")\n    test_dir_name = os.path.basename(os.path.dirname(test_path))\n    \n    os.makedirs(submission_folder, exist_ok=True)\n    \n    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n    \n    test_graph_ids = list(range(len(predictions)))\n    output_df = pd.DataFrame({\n        \"id\": test_graph_ids,\n        \"pred\": predictions\n    })\n    \n    output_df.to_csv(output_csv_path, index=False)\n    print(f\"Predictions saved to {output_csv_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_user_input(prompt, default=None, required=False, type_cast=str):\n\n    while True:\n        user_input = input(f\"{prompt} [{default}]: \")\n        \n        if user_input == \"\" and required:\n            print(\"This field is required. Please enter a value.\")\n            continue\n        \n        if user_input == \"\" and default is not None:\n            return default\n        \n        if user_input == \"\" and not required:\n            return None\n        \n        try:\n            return type_cast(user_input)\n        except ValueError:\n            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_arguments():\n    \"\"\"Set training configuration directly\"\"\"\n    args = {\n        # Dataset selection\n        'dataset': 'D',  # Choose: A, B, C, D\n        'train_mode': 1,  # 1=single dataset, 2=all datasets\n        \n        # Model config\n        #'gnn': 'gin',  # gin, gin-virtual, gcn, gcn-virtual\n        'num_layer': 3,\n        'emb_dim': 218,\n        'drop_ratio': 0.7,   # Dropout ratio\n        'virtual_node': True, # True to use virtual node, False otherwise\n        'residual': True,    # True to use residual connections, False otherwise\n        'JK': \"last\",         # Jumping Knowledge: \"last\", \"sum\", \"cat\"\n        'graph_pooling': \"mean\", # \"sum\", \"mean\", \"max\", \"attention\", \"set2set\"\n        'edge_drop_ratio' : 0.1,\n        'batch_norm' : True,\n        'layer_norm': False,\n        \n        # Training config\n        'batch_size': 64,\n        'epochs': 250,\n        'baseline_mode': 4,  # 1=CE, 2=Noisy CE, 3 GCE, 4 GCOD\n        'noise_prob': 0.2,\n        'gce_q' : 0.4,\n        'initial_lr' : 5e-3,\n\n        # Early stopping config\n        'early_stopping': True,  # Enable/disable early stopping\n        'patience': 25,\n        \n        # GCOD Loss Hyperparameters\n        'gcod_lambda_p': 2.0,    # Weight for prediction penalty in GCOD\n        'gcod_lambda_r': 0.1,    # Weight for u regularization in GCOD\n        'gcod_T_u': 15,           # Number of optimization iterations for u in GCOD\n        'gcod_lr_u': 0.1,       # Learning rate for optimizing u in GCOD\n\n        \n\n        # Lr scheduler config =================================================================================================================\n        'use_scheduler' : True,\n        'scheduler_type': 'ReduceLROnPlateau',  # Options: 'StepLR', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'ExponentialLR', 'OneCycleLR'\n\n        # StepLR parameters\n        'step_size': 30,      # Period of learning rate decay for StepLR\n        'gamma': 0.5,         # Multiplicative factor of learning rate decay\n        \n        # ReduceLROnPlateau parameters\n        'patience_lr': 10,    # Number of epochs with no improvement after which LR will be reduced\n        'factor': 0.5,        # Factor by which the learning rate will be reduced\n        'min_lr': 1e-7,       # Lower bound on the learning rate\n\n        \n        \n        # System config\n        'device': 0,\n        'num_checkpoints': 10\n    }\n    return argparse.Namespace(**args)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def populate_args(args):\n    print(\"Arguments received:\")\n    for key, value in vars(args).items():\n        print(f\"{key}: {value}\")\nargs = get_arguments()\npopulate_args(args)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NoisyCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, p_noisy):\n        super().__init__()\n        self.p = p_noisy\n        self.ce = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets):\n        losses = self.ce(logits, targets)\n        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n        return (losses * weights).mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GCOD vecchia","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GCODLoss(nn.Module):\n    \"\"\"\n    Graph Centroid Outlier Discounting (GCOD) Loss Function\n    Based on the NCOD method adapted for graph classification.\n    The model parameters (theta) are updated using L1 + L3.\n    The sample-specific parameters (u) are updated using L2.\n    \"\"\"\n    def __init__(self, num_classes, alpha_train=0.01):\n        \"\"\"\n        Args:\n            num_classes (int): Number of classes.\n            alpha_train (float): Corresponds to lambda_p in args, coefficient for the\n                                 feedback term in L1.\n        \"\"\"\n        super(GCODLoss, self).__init__()\n        self.num_classes = num_classes\n        self.alpha_train = alpha_train\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none') # for per-sample CE\n\n    def _ensure_u_shape(self, u_params, batch_size, target_ndim):\n        \"\"\"Helper to ensure u_params has the correct shape for operations.\"\"\"\n        if u_params.shape[0] != batch_size:\n            raise ValueError(f\"u_params batch dimension {u_params.shape[0]} does not match expected batch_size {batch_size}\")\n\n        if target_ndim == 1: # Expected shape [batch_size]\n            return u_params.squeeze() if u_params.ndim > 1 else u_params\n        elif target_ndim == 2: # Expected shape [batch_size, 1]\n            return u_params.unsqueeze(1) if u_params.ndim == 1 else u_params\n        return u_params\n\n\n    def compute_L1(self, logits, targets, u_params):\n        \"\"\"\n        Computes L1 = CE(f_θ(Z_B)) + α_train * u_B * (y_B ⋅ ỹ_B)\n        Args:\n            logits (Tensor): Model output logits, shape [batch_size, num_classes].\n            targets (Tensor): Ground truth labels, shape [batch_size].\n            u_params (Tensor): Per-sample u values for the batch, shape [batch_size] or [batch_size, 1].\n        Returns:\n            Tensor: Scalar L1 loss for the batch.\n        \"\"\"\n        batch_size = logits.size(0)\n        if batch_size == 0:\n            # Corrected line:\n            return torch.tensor(0.0, device=logits.device, requires_grad=logits.requires_grad)\n\n        y_onehot = F.one_hot(targets, num_classes=self.num_classes).float()\n        y_soft = F.softmax(logits, dim=1)\n\n        ce_loss_values = self.ce_loss(logits, targets) # Shape: [batch_size]\n\n        current_u_params = self._ensure_u_shape(u_params, batch_size, target_ndim=1)\n\n        feedback_term_values = self.alpha_train * current_u_params * (y_onehot * y_soft).sum(dim=1) # Shape: [batch_size]\n\n        L1 = ce_loss_values + feedback_term_values\n        return L1.mean()\n\n    def compute_L2(self, logits, targets, u_params):\n        \"\"\"\n        Computes L2 = (1/|C|) * ||ỹ_B + u_B * y_B - y_B||²\n        Args:\n            logits (Tensor): Model output logits, shape [batch_size, num_classes].\n            targets (Tensor): Ground truth labels, shape [batch_size].\n            u_params (Tensor): Per-sample u values for the batch, shape [batch_size] or [batch_size, 1].\n        Returns:\n            Tensor: Scalar L2 loss for the batch.\n        \"\"\"\n        batch_size = logits.size(0)\n        if batch_size == 0:\n            # Corrected line:\n            return torch.tensor(0.0, device=logits.device, requires_grad=logits.requires_grad)\n\n        y_onehot = F.one_hot(targets, num_classes=self.num_classes).float()\n        y_soft = F.softmax(logits, dim=1)\n\n        current_u_params_unsqueezed = self._ensure_u_shape(u_params, batch_size, target_ndim=2)\n\n        term = y_soft + current_u_params_unsqueezed * y_onehot - y_onehot # Shape: [batch_size, num_classes]\n\n        # L2 norm squared of the matrix 'term', then scaled\n        L2 = (1.0 / self.num_classes) * torch.norm(term, p='fro').pow(2) # Frobenius norm for matrix\n        return L2\n\n    def compute_L3(self, logits, targets, u_params, l3_coeff):\n        \"\"\"\n        Computes L3 = l3_coeff * D_KL(L || σ(-log(u_B)))\n                     where l3_coeff = (1 - training_accuracy)\n                     and L = log(σ(logit_true_class)) are log-probabilities\n                     and σ(-log(u_B)) are probabilities\n        Args:\n            logits (Tensor): Model output logits, shape [batch_size, num_classes].\n            targets (Tensor): Ground truth labels, shape [batch_size].\n            u_params (Tensor): Per-sample u values for the batch, shape [batch_size] or [batch_size, 1].\n            l3_coeff (float): Coefficient for the KL divergence term, e.g., (1 - training_accuracy).\n        Returns:\n            Tensor: Scalar L3 loss for the batch.\n        \"\"\"\n        batch_size = logits.size(0)\n        if batch_size == 0:\n            # Corrected line:\n            return torch.tensor(0.0, device=logits.device, requires_grad=logits.requires_grad)\n\n        y_onehot = F.one_hot(targets, num_classes=self.num_classes).float()\n\n        # Logit of the true class for each sample in the batch\n        diag_elements = (logits * y_onehot).sum(dim=1) # Shape: [batch_size]\n\n        # L_log_probs = log(sigma(true_class_logit)) which are log-probabilities\n        L_log_probs = F.logsigmoid(diag_elements) # Shape: [batch_size]\n\n        current_u_params = self._ensure_u_shape(u_params, batch_size, target_ndim=1)\n\n        # target_probs_for_kl = sigma(-log(u_B)) which are probabilities\n        target_probs_for_kl = torch.sigmoid(-torch.log(current_u_params + 1e-8)) # Shape: [batch_size]\n\n        # F.kl_div expects input (L_log_probs) as log-probabilities and target (target_probs_for_kl) as probabilities.\n        # reduction='mean' averages the loss over all elements in the batch.\n        # log_target=False means target_probs_for_kl are probabilities, not log-probabilities.\n        kl_div = F.kl_div(L_log_probs, target_probs_for_kl, reduction='mean', log_target=False)\n\n        L3 = l3_coeff * kl_div\n        return L3\n\n    def forward(self, logits, targets, u_params, training_accuracy):\n        \"\"\"\n        Calculates the GCOD loss components.\n        The main loss for model (theta) update is L1 + L3.\n        L2 is primarily used for updating u_params (called separately).\n        Args:\n            logits (Tensor): Model output logits.\n            targets (Tensor): Ground truth labels.\n            u_params (Tensor): Per-sample u values for the batch.\n            training_accuracy (float): The actual training accuracy (value between 0 and 1)\n                                     for the current batch or epoch.\n        Returns:\n            tuple: (total_loss_for_theta, L1, L2, L3)\n                   total_loss_for_theta = L1 + L3\n        \"\"\"\n        calculated_L1 = self.compute_L1(logits, targets, u_params)\n        # L2 is calculated here mainly for complete reporting if needed,\n        # but the train loop will call compute_L2 separately for u-optimization.\n        calculated_L2 = self.compute_L2(logits, targets, u_params)\n\n        l3_coefficient = (1.0 - training_accuracy) # As per GCOD paper (1 - alpha_train where alpha_train is accuracy)\n        calculated_L3 = self.compute_L3(logits, targets, u_params, l3_coefficient)\n\n        total_loss_for_theta = calculated_L1 + calculated_L3\n\n        return total_loss_for_theta, calculated_L1, calculated_L2, calculated_L3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GCOD nuova","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GCODLoss(nn.Module):\n    \"\"\"\n    Graph Centroid Outlier Discounting (GCOD) Loss Function\n    Based on the NCOD method adapted for graph classification.\n    The model parameters (theta) are updated using L1 + L3.\n    The sample-specific parameters (u) are updated using L2.\n    \"\"\"\n    def __init__(self, num_classes, alpha_train=0.01, lambda_r=0.1): # Added lambda_r\n        \"\"\"\n        Args:\n            num_classes (int): Number of classes.\n            alpha_train (float): Corresponds to lambda_p in args, coefficient for the\n                                 feedback term in L1.\n            lambda_r (float): Coefficient for the u regularization term in L2.\n        \"\"\"\n        super(GCODLoss, self).__init__()\n        self.num_classes = num_classes\n        self.alpha_train = alpha_train\n        self.lambda_r = lambda_r # Store lambda_r\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none') # for per-sample CE\n\n    def _ensure_u_shape(self, u_params, batch_size, target_ndim):\n        \"\"\"Helper to ensure u_params has the correct shape for operations.\"\"\"\n        if u_params.shape[0] != batch_size:\n            raise ValueError(f\"u_params batch dimension {u_params.shape[0]} does not match expected batch_size {batch_size}\")\n\n        if target_ndim == 1: # Expected shape [batch_size]\n            return u_params.squeeze() if u_params.ndim > 1 else u_params\n        elif target_ndim == 2: # Expected shape [batch_size, 1]\n            return u_params.unsqueeze(1) if u_params.ndim == 1 else u_params\n        return u_params\n\n\n    def compute_L1(self, logits, targets, u_params):\n        \"\"\"\n        Computes L1 = CE(f_θ(Z_B)) + α_train * u_B * (y_B ⋅ ỹ_B)\n        Args:\n            logits (Tensor): Model output logits, shape [batch_size, num_classes].\n            targets (Tensor): Ground truth labels, shape [batch_size].\n            u_params (Tensor): Per-sample u values for the batch, shape [batch_size] or [batch_size, 1].\n        Returns:\n            Tensor: Scalar L1 loss for the batch.\n        \"\"\"\n        batch_size = logits.size(0)\n        if batch_size == 0:\n            # Corrected line:\n            return torch.tensor(0.0, device=logits.device, requires_grad=logits.requires_grad)\n\n        y_onehot = F.one_hot(targets, num_classes=self.num_classes).float()\n        y_soft = F.softmax(logits, dim=1)\n\n        ce_loss_values = self.ce_loss(logits, targets) # Shape: [batch_size]\n\n        current_u_params = self._ensure_u_shape(u_params, batch_size, target_ndim=1)\n\n        feedback_term_values = self.alpha_train * current_u_params * (y_onehot * y_soft).sum(dim=1) # Shape: [batch_size]\n\n        L1 = ce_loss_values + feedback_term_values\n        return L1.mean()\n\n    def compute_L2(self, logits, targets, u_params):\n        \"\"\"\n        Computes L2 = (1/|C|) * ||ỹ_B + u_B * y_B - y_B||²_F + λ_r * ||u_B||²_2\n        Args:\n            logits (Tensor): Model output logits, shape [batch_size, num_classes].\n            targets (Tensor): Ground truth labels, shape [batch_size].\n            u_params (Tensor): Per-sample u values for the batch, shape [batch_size] or [batch_size, 1].\n        Returns:\n            Tensor: Scalar L2 loss for the batch (for u optimization).\n        \"\"\"\n        batch_size = logits.size(0)\n        if batch_size == 0:\n            # Corrected line:\n            return torch.tensor(0.0, device=logits.device, requires_grad=logits.requires_grad)\n\n        y_onehot = F.one_hot(targets, num_classes=self.num_classes).float()\n        y_soft = F.softmax(logits, dim=1)\n\n        current_u_params_unsqueezed = self._ensure_u_shape(u_params, batch_size, target_ndim=2)\n\n        term = y_soft + current_u_params_unsqueezed * y_onehot - y_onehot # Shape: [batch_size, num_classes]\n\n        # L2 reconstruction term (Frobenius norm for matrix part)\n        L2_reconstruction = (1.0 / self.num_classes) * torch.norm(term, p='fro').pow(2)\n        \n        # u regularization term (L2 norm for u_params vector part)\n        # Ensure u_params is 1D for this norm calculation\n        current_u_params_1d = self._ensure_u_shape(u_params, batch_size, target_ndim=1)\n        u_reg = self.lambda_r * torch.norm(current_u_params_1d, p=2).pow(2)\n\n        L2 = L2_reconstruction + u_reg\n        return L2\n\n    def compute_L3(self, logits, targets, u_params, l3_coeff):\n        \"\"\"\n        Computes L3 = l3_coeff * D_KL(L || σ(-log(u_B)))\n                     where l3_coeff = (1 - training_accuracy)\n                     and L = log(σ(logit_true_class)) are log-probabilities\n                     and σ(-log(u_B)) are probabilities\n        Args:\n            logits (Tensor): Model output logits, shape [batch_size, num_classes].\n            targets (Tensor): Ground truth labels, shape [batch_size].\n            u_params (Tensor): Per-sample u values for the batch, shape [batch_size] or [batch_size, 1].\n            l3_coeff (float): Coefficient for the KL divergence term, e.g., (1 - training_accuracy).\n        Returns:\n            Tensor: Scalar L3 loss for the batch.\n        \"\"\"\n        batch_size = logits.size(0)\n        if batch_size == 0:\n            # Corrected line:\n            return torch.tensor(0.0, device=logits.device, requires_grad=logits.requires_grad)\n\n        y_onehot = F.one_hot(targets, num_classes=self.num_classes).float()\n\n        # Logit of the true class for each sample in the batch\n        diag_elements = (logits * y_onehot).sum(dim=1) # Shape: [batch_size]\n\n        # L_log_probs = log(sigma(true_class_logit)) which are log-probabilities\n        L_log_probs = F.logsigmoid(diag_elements) # Shape: [batch_size]\n\n        current_u_params = self._ensure_u_shape(u_params, batch_size, target_ndim=1)\n\n        # target_probs_for_kl = sigma(-log(u_B)) which are probabilities\n        target_probs_for_kl = torch.sigmoid(-torch.log(current_u_params + 1e-8)) # Shape: [batch_size]\n\n        # F.kl_div expects input (L_log_probs) as log-probabilities and target (target_probs_for_kl) as probabilities.\n        # reduction='mean' averages the loss over all elements in the batch.\n        # log_target=False means target_probs_for_kl are probabilities, not log-probabilities.\n        kl_div = F.kl_div(L_log_probs, target_probs_for_kl, reduction='mean', log_target=False)\n\n        L3 = l3_coeff * kl_div\n        return L3\n\n    def forward(self, logits, targets, u_params, training_accuracy):\n        \"\"\"\n        Calculates the GCOD loss components.\n        The main loss for model (theta) update is L1 + L3.\n        L2 is primarily used for updating u_params (called separately).\n        Args:\n            logits (Tensor): Model output logits.\n            targets (Tensor): Ground truth labels.\n            u_params (Tensor): Per-sample u values for the batch.\n            training_accuracy (float): The actual training accuracy (value between 0 and 1)\n                                     for the current batch or epoch.\n        Returns:\n            tuple: (total_loss_for_theta, L1, L2, L3)\n                   total_loss_for_theta = L1 + L3\n        \"\"\"\n        calculated_L1 = self.compute_L1(logits, targets, u_params)\n        # L2 is calculated here mainly for complete reporting if needed,\n        # but the train loop will call compute_L2 separately for u-optimization.\n        # This L2 will now include the regularization term.\n        calculated_L2 = self.compute_L2(logits, targets, u_params)\n\n        l3_coefficient = (1.0 - training_accuracy) # As per GCOD paper (1 - alpha_train where alpha_train is accuracy)\n        calculated_L3 = self.compute_L3(logits, targets, u_params, l3_coefficient)\n\n        total_loss_for_theta = calculated_L1 + calculated_L3\n\n        return total_loss_for_theta, calculated_L1, calculated_L2, calculated_L3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"Enhanced GNN Training Pipeline\")\nprint(\"=\" * 60)\n\n# Get configuration\nargs = get_arguments()\n\nprint(\"\\nConfiguration:\")\nfor key, value in vars(args).items():\n    print(f\"  {key}: {value}\")\n\n# Setup device\ndevice = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nUsing device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\\\n\" + \"=\"*40)\nprint(\"LOADING DATA\")\nprint(\"=\"*40)\n\nbase_path = '/kaggle/input/deep-dataset-preprocessed/processed_data_separate'\n\n# Prepare training/validation data based on mode\nif args.train_mode == 1:\n    # Single dataset mode\n    dataset_name = args.dataset\n    \n    loaded_train_graphs = torch.load(f'{base_path}/{dataset_name}_train_graphs.pt', weights_only=False)\n    train_dataset_with_indices = []\n    for i, data_item in enumerate(loaded_train_graphs):\n        data_item = add_zeros(data_item)\n        data_item.original_idx = i # Store original index\n        train_dataset_with_indices.append(data_item)\n    train_dataset = train_dataset_with_indices\n    \n    loaded_val_graphs = torch.load(f'{base_path}/{dataset_name}_val_graphs.pt', weights_only=False)\n    val_dataset_with_indices = []\n    for i, data_item in enumerate(loaded_val_graphs):\n        data_item = add_zeros(data_item)\n        data_item.original_idx = i # Store original index for val set too if needed by u_params in eval\n        val_dataset_with_indices.append(data_item)\n    val_dataset = val_dataset_with_indices\n    \n    # Test dataset usually doesn't need original_idx for u_params update\n    test_dataset = torch.load(f'{base_path}/{dataset_name}_test_graphs.pt', weights_only=False)\n    test_dataset = [add_zeros(data) for data in test_dataset] # original_idx not strictly needed for test\n    \n    print(f\"Using single dataset: {dataset_name}\")\nelse:\n    # All datasets mode\n    train_dataset_with_indices = []\n    val_dataset_with_indices = []\n    current_train_idx = 0\n    current_val_idx = 0\n\n    for ds_name in ['A', 'B', 'C', 'D']:\n        loaded_train_ds = torch.load(f'{base_path}/{ds_name}_train_graphs.pt', weights_only=False)\n        for data_item in loaded_train_ds:\n            data_item = add_zeros(data_item)\n            data_item.original_idx = current_train_idx\n            train_dataset_with_indices.append(data_item)\n            current_train_idx += 1\n            \n        loaded_val_ds = torch.load(f'{base_path}/{ds_name}_val_graphs.pt', weights_only=False)\n        for data_item in loaded_val_ds:\n            data_item = add_zeros(data_item)\n            data_item.original_idx = current_val_idx\n            val_dataset_with_indices.append(data_item)\n            current_val_idx +=1\n            \n    train_dataset = train_dataset_with_indices\n    val_dataset = val_dataset_with_indices\n    \n    test_dataset = torch.load(f'{base_path}/{args.dataset}_test_graphs.pt', weights_only=False)\n    test_dataset = [add_zeros(data) for data in test_dataset] # original_idx not strictly needed for test\n    print(\"Using all datasets for training\")\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model setup","metadata":{}},{"cell_type":"markdown","source":"Scheduler setup","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"MODEL SETUP\")\nprint(\"=\"*40)\n\n# Initialize model\nmodel = GNN(num_class=6, # Assuming 6 classes based on original notebook\n            num_layer=args.num_layer,\n            emb_dim=args.emb_dim,\n            drop_ratio=args.drop_ratio,\n            virtual_node=args.virtual_node,\n            residual=args.residual,\n            JK=args.JK,\n            graph_pooling=args.graph_pooling,\n            edge_drop_ratio = args.edge_drop_ratio,\n            batch_norm=args.batch_norm\n           )\n\nmodel = model.to(device)\n\n# Setup optimizer and loss\n#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nif args.baseline_mode == 2:\n    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n    print(f\"Using Noisy Cross Entropy Loss (p={args.noise_prob})\")\nelif args.baseline_mode == 3: # <--- ADD THIS BLOCK FOR GCE\n    criterion = GeneralizedCrossEntropyLoss(q=args.gce_q)\n    print(f\"Using Generalized Cross Entropy (GCE) Loss (q={args.gce_q})\")\nelif args.baseline_mode == 4: # GCOD Loss\n    criterion = GCODLoss(\n        num_classes=6, # Assuming 6 classes\n        alpha_train=args.gcod_lambda_p, # Map gcod_lambda_p to alpha_train\n        lambda_r=args.gcod_lambda_r    # Pass gcod_lambda_r for u regularization\n    )\n    # Updated print statement to reflect lambda_r is used\n    print(f\"Using GCOD Loss. Effective alpha_train (lambda_p for L1)={args.gcod_lambda_p}, \"\n          f\"lambda_r (for u-regularization in L2)={args.gcod_lambda_r}, \"\n          f\"T_u={args.gcod_T_u}, lr_u={args.gcod_lr_u}\")\nelse:\n    criterion = torch.nn.CrossEntropyLoss()\n    print(\"Using standard Cross Entropy Loss\")\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Setup logging and checkpoints\n#exp_name = f\"{args.gnn}_dataset{args.dataset}_mode{args.train_mode}\"\nexp_name = f\"gin_dataset{args.dataset}_mode{args.train_mode}\"\nlogs_dir = os.path.join(\"logs\", exp_name)\ncheckpoints_dir = os.path.join(\"checkpoints\", exp_name)\nos.makedirs(logs_dir, exist_ok=True)\nos.makedirs(checkpoints_dir, exist_ok=True)\n\n# Setup logging\nlog_file = os.path.join(logs_dir, \"training.log\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\n\n#best_model_path = os.path.join(checkpoints_dir, \"best_model.pth\")\nbest_model_path = '/kaggle/working/checkpoints/best_model.pth'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning Rate Scheduler Setup\nprint(\"\\n\" + \"=\"*40)\nprint(\"SCHEDULER SETUP\")\nprint(\"=\"*40)\n\n# Update optimizer with initial learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=args.initial_lr)\n\nscheduler = None\nif args.use_scheduler:\n    if args.scheduler_type == 'StepLR':\n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, \n            step_size=args.step_size, \n            gamma=args.gamma\n        )\n        print(f\"Using StepLR scheduler: step_size={args.step_size}, gamma={args.gamma}\")\n        \n    elif args.scheduler_type == 'ReduceLROnPlateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='max',  # We want to reduce LR when validation loss stops decreasing\n            factor=args.factor,\n            patience=args.patience_lr,\n            min_lr=args.min_lr,\n        )\n        print(f\"Using ReduceLROnPlateau scheduler: factor={args.factor}, patience={args.patience_lr}, min_lr={args.min_lr}\")\n        \n    else:\n        print(f\"Unknown scheduler type: {args.scheduler_type}. No scheduler will be used.\")\n        args.use_scheduler = False\nelse:\n    print(\"No learning rate scheduler will be used.\")\n\nprint(f\"Initial learning rate: {args.initial_lr}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train loop","metadata":{}},{"cell_type":"code","source":"print(\"\\\\n\" + \"=\"*40)\nprint(\"TRAINING\")\nprint(\"=\"*40)\n\n# Initialize u_values_for_train here, after train_dataset is fully formed (from Step 1)\nu_values_for_train = None\nif args.baseline_mode == 4:\n    if 'train_dataset' in locals() and len(train_dataset) > 0 : # Check if train_dataset exists\n        u_values_for_train = torch.zeros(len(train_dataset), device=device, requires_grad=False) # on main device\n        print(f\"Initialized u_values_for_train for GCOD with size: {u_values_for_train.size()}\")\n    else:\n        print(\"Warning: train_dataset not found or empty when trying to initialize u_values_for_train for GCOD.\")\n\nbest_val_accuracy = 0.0\ntrain_losses_list = [] # Renamed\ntrain_accuracies_list = [] # Renamed\nval_losses_list = [] # Renamed\nval_accuracies_list = [] # Renamed\nlearning_rates = []\n\n# Early stopping variables\nif args.early_stopping:\n    epochs_without_improvement = 0\n    print(f\"Early stopping enabled with patience: {args.patience}\")\nelse:\n    print(\"Early stopping disabled\")\n\n# Calculate checkpoint intervals\nif args.num_checkpoints > 1:\n    checkpoint_intervals = [int((i + 1) * args.epochs / args.num_checkpoints) \n                          for i in range(args.num_checkpoints)]\nelse:\n    checkpoint_intervals = [args.epochs]\n\nfor epoch in range(args.epochs):\n    print(f\"\\\\nEpoch {epoch + 1}/{args.epochs}\")\n    print(\"-\" * 30)\n\n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    learning_rates.append(current_lr)\n    \n    # Training\n    train_loss, train_acc = train(\n        train_loader, model, optimizer, criterion, device,\n        save_checkpoints=(epoch + 1 in checkpoint_intervals), # This was inside train, moved out\n        checkpoint_path=os.path.join(checkpoints_dir, \"checkpoint\"),\n        current_epoch=epoch,\n        args_namespace=args, # Pass the whole args namespace\n        u_values_global=u_values_for_train, # Pass global u_values\n        current_baseline_mode=args.baseline_mode, # Pass baseline_mode\n        scheduler=args.scheduler_type\n    )\n    \n    # Validation\n    val_loss, val_acc = evaluate(\n        val_loader, model, criterion, device, calculate_accuracy=True,\n        args_namespace=args, # Pass args for baseline_mode check\n        u_values_global_eval=None # Pass u_values_for_val if you implement using them, else dummy zeros are used\n    )    \n    # Log results\n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    print(f\"Learning Rate: {current_lr:.2e}\")\n    \n    \n    logging.info(f\"Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n                f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, LR={current_lr:.2e}\")\n    \n    # Store metrics (existing code using new list names)\n    train_losses_list.append(train_loss)\n    train_accuracies_list.append(train_acc)\n    val_losses_list.append(val_loss)\n    val_accuracies_list.append(val_acc)\n    \n    # Save best model\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"★ New best model saved! Val Acc: {val_acc:.4f}\")\n\n\n        # Reset early stopping counter\n        if args.early_stopping:\n            epochs_without_improvement = 0\n\n    else:\n        # No improvement\n        if args.early_stopping:\n            epochs_without_improvement += 1\n            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n            \n            # Check if we should stop early\n            if epochs_without_improvement >= args.patience:\n                print(f\"\\nEarly stopping triggered! No improvement for {args.patience} epochs.\")\n                print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n                break\n\n    # Learning rate scheduler step\n    if scheduler is not None:\n        if args.scheduler_type == 'ReduceLROnPlateau':\n            # ReduceLROnPlateau needs the metric to monitor\n            scheduler.step(val_acc)\n        elif args.scheduler_type == 'OneCycleLR':\n            # OneCycleLR steps every batch, not every epoch\n            # This is handled in the training function\n            pass\n        else:\n            # Other schedulers step every epoch\n            scheduler.step()\n        \n        # Check if learning rate changed\n        new_lr = optimizer.param_groups[0]['lr']\n        if new_lr != current_lr:\n            print(f\"Learning rate changed: {current_lr:.2e} → {new_lr:.2e}\")\n    \nprint(f\"\\nBest validation accuracy: {best_val_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot","metadata":{}},{"cell_type":"code","source":"plot_training_progress(train_losses_list, train_accuracies_list, val_losses_list, val_accuracies_list, logs_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*40)\nprint(\"TESTING\")\nprint(\"=\"*40)\n\n# Load best model and make predictions\nmodel.load_state_dict(torch.load(best_model_path))\nprint(f\"Loaded best model from: {best_model_path}\")\n\n#predictions = evaluate(test_loader, model, criterion, device, calculate_accuracy=False)\n\n# Save predictions\npredictions = evaluate(\n    test_loader, model, criterion, device, calculate_accuracy=False,\n    args_namespace=args, # For consistency, though not used if calculate_accuracy=False and GCOD loss not computed\n    u_values_global_eval=None # Not relevant for test set predictions without loss\n)\n\n# Save predictions\nsave_predictions(predictions,f\"/kaggle/working/submission/testset_{args.dataset}.json\")\n\n# Cleanup for memory\ndel train_dataset, val_dataset, test_dataset\ndel train_loader, val_loader, test_loader\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*60)\nprint(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\nprint(f\"Predictions saved for dataset {args.dataset}\")\nprint(f\"Logs and plots saved in: {logs_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}